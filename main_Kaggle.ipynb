{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814},{"sourceType":"datasetVersion","sourceId":12412923,"datasetId":7828532}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:17:53.475150Z","iopub.execute_input":"2025-07-08T18:17:53.475416Z","iopub.status.idle":"2025-07-08T18:17:54.525848Z","shell.execute_reply.started":"2025-07-08T18:17:53.475390Z","shell.execute_reply":"2025-07-08T18:17:54.525177Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/glove.6B.100d.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"<h1>Our Primary Objective is to Understand the Pure Performance of Different RNN Architectures on Raw Text Data</h1>","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader , Dataset\nimport torch\nimport torch.nn.functional as F\nfrom torchmetrics import Accuracy, Precision, Recall, F1Score\nfrom torchmetrics.classification import BinaryConfusionMatrix # Specifically for binary classification\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nimport os\nimport shutil\nfrom datasets import load_from_disk\n\nfrom datasets import load_dataset\nfrom datasets import concatenate_datasets, DatasetDict\nimport time","metadata":{"execution":{"iopub.status.busy":"2025-07-08T18:19:22.040035Z","iopub.execute_input":"2025-07-08T18:19:22.040548Z","iopub.status.idle":"2025-07-08T18:19:34.806866Z","shell.execute_reply.started":"2025-07-08T18:19:22.040523Z","shell.execute_reply":"2025-07-08T18:19:34.806118Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from collections import Counter\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS as DEFAULT_STOP_WORDS\n\n# Ensure SpaCy is loaded\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept OSError:\n    spacy.cli.download(\"en_core_web_sm\")\n    nlp = spacy.load(\"en_core_web_sm\")\n","metadata":{"execution":{"iopub.status.busy":"2025-07-08T18:19:38.766276Z","iopub.execute_input":"2025-07-08T18:19:38.766828Z","iopub.status.idle":"2025-07-08T18:19:39.296761Z","shell.execute_reply.started":"2025-07-08T18:19:38.766808Z","shell.execute_reply":"2025-07-08T18:19:39.296178Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"\n\n\ndataset_name = \"stanfordnlp/imdb\"\n\ndataset = load_dataset(dataset_name)\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\n\nfull_dataset = concatenate_datasets([train_dataset, test_dataset])\nprint(len(full_dataset))\n","metadata":{"execution":{"iopub.status.busy":"2025-07-08T18:19:42.929446Z","iopub.execute_input":"2025-07-08T18:19:42.929636Z","iopub.status.idle":"2025-07-08T18:19:44.146277Z","shell.execute_reply.started":"2025-07-08T18:19:42.929621Z","shell.execute_reply":"2025-07-08T18:19:44.145675Z"},"trusted":true},"outputs":[{"name":"stdout","text":"50000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Train Test Validation Splits\n\nTo know the right ratio of splits and what method to use for train_test_split.. \n\n- Check whether dataset is balanced , imbalanced ? If it is imbalanced then we should use stratified splitting to ensure that each split (train, val, test) maintains the same class distribution as the original dataset.\n- Since dataset is moderately size around 50000, (80,10,10) split works and gives around 5000 for testing and validation which are quite enough","metadata":{}},{"cell_type":"code","source":" # 1. Extract all labels\nlabels = full_dataset['label']\n\n# 2. Count the occurrences of each label\nlabel_counts = Counter(labels)\n\nprint(\"Class Distribution:\")\ntotal_samples = len(full_dataset)\n\n# 3. Print counts and percentages\nfor label, count in sorted(label_counts.items()):\n    percentage = (count / total_samples) * 100\n    print(f\"  Label {label}: {count} samples ({percentage:.2f}%)\")\n\n# 4. Determine if it's balanced\n# A common heuristic for \"imbalanced\" is if the ratio of the majority to minority class\n# is significantly high (e.g., 2:1, 5:1, 10:1 or more).\n# For simplicity, we'll just check if there's a large variance in percentages.\nif len(label_counts) > 1:\n    min_percentage = min(label_counts.values()) / total_samples * 100\n    max_percentage = max(label_counts.values()) / total_samples * 100\n\n    if max_percentage / min_percentage > 1.5: # Adjust this threshold as needed\n        print(\"\\nConclusion: The dataset appears to be imbalanced.\")\n        print(f\"  Ratio of majority to minority class: {max_percentage:.2f}% / {min_percentage:.2f}% = {max_percentage / min_percentage:.2f}x\")\n    else:\n        print(\"\\nConclusion: The dataset appears to be relatively balanced.\")\nelse:\n    print(\"\\nConclusion: Only one class found. The dataset has no imbalance (or is a single-class dataset).\")","metadata":{"execution":{"iopub.execute_input":"2025-07-08T17:25:36.450663Z","iopub.status.busy":"2025-07-08T17:25:36.450446Z","iopub.status.idle":"2025-07-08T17:25:36.476531Z","shell.execute_reply":"2025-07-08T17:25:36.475669Z","shell.execute_reply.started":"2025-07-08T17:25:36.450647Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Class Distribution:\n","  Label 0: 25000 samples (50.00%)\n","  Label 1: 25000 samples (50.00%)\n","\n","Conclusion: The dataset appears to be relatively balanced.\n"]}],"execution_count":5},{"cell_type":"markdown","source":"Since dataset is balanced, we can use normal splitting ","metadata":{}},{"cell_type":"code","source":" #Step 1: Split into 90% (train+val) and 10% (test)\n\ntrain_val_test_split = full_dataset.train_test_split(test_size=0.1, seed=42)\n\nfinal_test_dataset = train_val_test_split['test']\ntrain_val_dataset = train_val_test_split['train']\n\n# Step 2: Split the 90% (train+val) into 8/9 for train and 1/9 for validation\ntrain_val_split = train_val_dataset.train_test_split(test_size=(0.1/0.9), seed=42)\n\nfinal_train_dataset = train_val_split['train']\nfinal_validation_dataset = train_val_split['test'] # This is your validation set\n\n\n# Combine into a DatasetDict for convenience\nfinal_splits = DatasetDict({\n    'train': final_train_dataset,\n    'validation': final_validation_dataset,\n    'test': final_test_dataset\n})\n\nprint(f\"Original dataset size: {len(full_dataset)}\")\nprint(f\"Train dataset size: {len(final_splits['train'])}\")\nprint(f\"Validation dataset size: {len(final_splits['validation'])}\")\nprint(f\"Test dataset size: {len(final_splits['test'])}\")\n\n# Verify the approximate ratios\ntotal_samples = len(full_dataset)\nprint(f\"Train ratio: {len(final_splits['train']) / total_samples:.2f}\")\nprint(f\"Validation ratio: {len(final_splits['validation']) / total_samples:.2f}\")\nprint(f\"Test ratio: {len(final_splits['test']) / total_samples:.2f}\")\n\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-07-08T18:19:44.189347Z","iopub.execute_input":"2025-07-08T18:19:44.189561Z","iopub.status.idle":"2025-07-08T18:19:44.203190Z","shell.execute_reply.started":"2025-07-08T18:19:44.189546Z","shell.execute_reply":"2025-07-08T18:19:44.202652Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Original dataset size: 50000\nTrain dataset size: 40000\nValidation dataset size: 5000\nTest dataset size: 5000\nTrain ratio: 0.80\nValidation ratio: 0.10\nTest ratio: 0.10\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"###### A smaller dataset for convenience of training \n\n\n\n\n\nratio = 1\n\nnum_train_samples = len(final_train_dataset)\nsmall_train_size = int(num_train_samples * ratio)\nsmall_train_dataset = final_train_dataset.select(range(small_train_size))\n\n\nnum_test_samples = len(final_test_dataset)\nsmall_test_size = int(num_test_samples * ratio)\nsmall_test_dataset = final_test_dataset.select(range(small_test_size))\n\n\n\nnum_validation_samples = len(final_validation_dataset)\nsmall_validation_size = int(num_test_samples * ratio)\nsmall_validation_dataset = final_test_dataset.select(range(small_test_size))\n\n\nprint(f\"Small train dataset size : {len(small_train_dataset)}\")\nprint(f\"Small test dataset size : {len(small_test_dataset)}\")\nprint(f\"Small validation dataset size : {len(small_validation_dataset)}\")\n\n\nfinal_splits_reduced_dataset = DatasetDict({\n    'train': small_train_dataset,\n    'validation': small_validation_dataset,\n    'test': small_test_dataset\n})\nfinal_splits_reduced_dataset","metadata":{"execution":{"iopub.status.busy":"2025-07-08T18:19:45.899699Z","iopub.execute_input":"2025-07-08T18:19:45.899987Z","iopub.status.idle":"2025-07-08T18:19:45.913583Z","shell.execute_reply.started":"2025-07-08T18:19:45.899965Z","shell.execute_reply":"2025-07-08T18:19:45.912965Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Small train dataset size : 40000\nSmall test dataset size : 5000\nSmall validation dataset size : 5000\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 40000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"\n# Tokenizing and Creating Embeddings of Dataset using glove","metadata":{}},{"cell_type":"markdown","source":"**Contractions are Tokens:** For sentiment analysis, contractions (like \"n't\", \"'s\") often carry significant meaning (negation, possession). Do NOT filter them out. Ensure they are treated as individual tokens and that your pre-trained embedding vocabulary contains entries for them. GloVe usually has entries for common contractions.\n\n**Consistency is Key:** Whatever tokenization and preprocessing rules you decide on (e.g., lowercasing, stop word removal, punctuation handling), apply them identically when building your vocabulary from the training data and when processing new text (validation/test sets).m","metadata":{}},{"cell_type":"markdown","source":"**Stop Words: A Decision:** Removing stop words (a, the, is, are) can reduce vocabulary size and noise, potentially speeding up training. However, sometimes they are crucial for sentiment (e.g., \"this is not good\"). For your first experiments, you might try with and without stop word removal to see the impact. SpaCy provides a good list of stop words.","metadata":{}},{"cell_type":"markdown","source":"### Customizing the tokenizing scheme for our task for sentiment classification","metadata":{}},{"cell_type":"code","source":"\n\ntext = \" <> I do not like 6  :)  this movie at all no 100. It was never truly bad, but also not amazing. No, I don't feel good about it.\"\n\nprint(\"--- Default SpaCy Stop Word List ---\")\nprint(f\"Contains 'not': {'not' in DEFAULT_STOP_WORDS}\")\nprint(f\"Contains 'no': {'no' in DEFAULT_STOP_WORDS}\")\nprint(f\"Contains 'never': {'never' in DEFAULT_STOP_WORDS}\")\nprint(f\"Contains 'do': {'do' in DEFAULT_STOP_WORDS}\") # 'do' often is\nprint(f\"Contains 'all': {'all' in DEFAULT_STOP_WORDS}\") # 'at all' can be intensifying\n\n# --- Step 1: Define Negation and Sentiment-Critical Words to KEEP ---\n# This list can be expanded based on your domain and observation.\nNEGATION_WORDS_TO_KEEP = {\n    \"not\", \"no\", \"never\", \"none\", \"nothing\", \"nowhere\", \"hardly\", \"scarcely\", \"barely\",\n    \"don't\", \"doesn't\", \"didn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\",\n    \"won't\", \"wouldn't\", \"can't\", \"couldn't\", \"shouldn't\", \"mightn't\", \"mustn't\"\n}\n\n\n\n# --- Step 2: Create a Custom Stop Word List ---\n# Remove our negation words from the default SpaCy stop words\nCUSTOM_STOP_WORDS = DEFAULT_STOP_WORDS - NEGATION_WORDS_TO_KEEP\n\nprint(\"\\n--- Custom Stop Word List Check ---\")\nprint(f\"Custom stop words contain 'not': {'not' in CUSTOM_STOP_WORDS}\")\nprint(f\"Custom stop words contain 'no': {'no' in CUSTOM_STOP_WORDS}\")\nprint(f\"Custom stop words contain 'never': {'never' in CUSTOM_STOP_WORDS}\")\nprint(f\"Custom stop words contain 'do': {'do' in CUSTOM_STOP_WORDS}\") # 'do' might still be there if not in NEGATION_WORDS_TO_KEEP\n\n# --- Step 3: Apply Custom Stop Word List During Tokenization and Filtering ---\ndef custom_tokenize_and_filter(text_input, nlp_model, stop_words_set, include_emoticons=True):\n    doc = nlp_model(text_input)\n    processed_tokens = []\n    for token in doc:\n        token_text_lower = token.text.lower()\n        \n        # Rule 1: Filter out standalone punctuation and spaces\n        if token.is_punct or token.is_space:\n            # Rule 1a: Optional - keep specific emoticons even if they are punctuation\n            if include_emoticons and token_text_lower in [\":-)\", \":)\", \":(\", \";)\", \":P\", \":O\", \":|\"]:\n                processed_tokens.append(token_text_lower)\n            continue # Skip other punctuation and spaces\n\n        # Rule 2: Filter out stop words (but only if NOT in our kept negation list)\n        if token_text_lower in stop_words_set:\n            continue # Skip stop words\n\n        # Rule 3: Keep numbers as is (or replace with [NUM] if preferred)\n        # For sentiment, keeping them is often fine, unless they are just noise.\n        if token.like_num:\n           processed_tokens.append(\"<NUM>\")\n  \n        else:\n         processed_tokens.append(token_text_lower)\n            \n    return processed_tokens\n\nprint(\"\\n--- Tokenization with Custom Stop Words ---\")\ntokens_processed = custom_tokenize_and_filter(text, nlp, CUSTOM_STOP_WORDS, include_emoticons=True)\nprint(f\"Original: {text}\")\nprint(f\"Processed tokens: {tokens_processed}\")\n\n# Demonstrate with another example for clarity\ntext_positive = \"This is really good, I absolutely love it!\"\ntokens_positive = custom_tokenize_and_filter(text_positive, nlp, CUSTOM_STOP_WORDS)\nprint(f\"\\nOriginal: {text_positive}\")\nprint(f\"Processed tokens: {tokens_positive}\")\n\ntext_negative = \"I hate this, it's not at all enjoyable. Never again.\"\ntokens_negative = custom_tokenize_and_filter(text_negative, nlp, CUSTOM_STOP_WORDS)\nprint(f\"\\nOriginal: {text_negative}\")\nprint(f\"Processed tokens: {tokens_negative}\")\n\n# --- Vocabulary Building (Example using this custom function) ---\ndummy_texts_large = [\n    \"This is a great movie no, I love it!\",\n    \"The acting was terrible and the plot was boring.\",\n    \"A wonderful experience, highly recommended.\",\n    \"Not bad, but could be better.\",\n    \"This movie is fantastic and amazing.\",\n    \"The director's vision was inspiring and unique.  3 7\",\n    \"Absolutely horrendous, a complete waste of time.\",\n    \"An uplifting and heartwarming story.\",\n    \"Could not stop laughing, truly hilarious.\",\n    \"I don't like this film at all.6 9 \"\n] * 10 # Simulating more data\n\nall_tokens_vocab_build = []\nfor t in dummy_texts_large:\n    processed_sentence_tokens = custom_tokenize_and_filter(t, nlp, CUSTOM_STOP_WORDS)\n    all_tokens_vocab_build.extend(processed_sentence_tokens)\n\ntoken_counts_vocab_build = Counter(all_tokens_vocab_build)\n\nPAD_TOKEN = '<PAD>'\nUNK_TOKEN = '<UNK>'\nword_to_idx = {PAD_TOKEN: 0, UNK_TOKEN: 1}\ncurrent_idx = 2\nfor word, count in token_counts_vocab_build.most_common(): # Use all words from your dataset for the vocab\n    if word not in word_to_idx:\n        word_to_idx[word] = current_idx\n        current_idx += 1\n\nVOCAB_SIZE_YOUR_DATA = len(word_to_idx)\nprint(f\"\\nYour dataset's vocabulary size after smarter processing with custom stopwords: {VOCAB_SIZE_YOUR_DATA}\")\nprint(f\"Word to index for 'not': {word_to_idx.get('not')}\") # Should have an index\nprint(f\"Word to index for 'no': {word_to_idx.get('no')}\")   # Should have an index\nprint(f\"Word to index for 'is': {word_to_idx.get('is')}\")   # Should be None if removed\n\n","metadata":{"execution":{"iopub.status.busy":"2025-07-08T18:19:53.411382Z","iopub.execute_input":"2025-07-08T18:19:53.411924Z","iopub.status.idle":"2025-07-08T18:19:53.928133Z","shell.execute_reply.started":"2025-07-08T18:19:53.411903Z","shell.execute_reply":"2025-07-08T18:19:53.927503Z"},"trusted":true},"outputs":[{"name":"stdout","text":"--- Default SpaCy Stop Word List ---\nContains 'not': True\nContains 'no': True\nContains 'never': True\nContains 'do': True\nContains 'all': True\n\n--- Custom Stop Word List Check ---\nCustom stop words contain 'not': False\nCustom stop words contain 'no': False\nCustom stop words contain 'never': False\nCustom stop words contain 'do': True\n\n--- Tokenization with Custom Stop Words ---\nOriginal:  <> I do not like 6  :)  this movie at all no 100. It was never truly bad, but also not amazing. No, I don't feel good about it.\nProcessed tokens: ['<', '>', 'not', 'like', '<NUM>', ':)', 'movie', 'no', '<NUM>', 'never', 'truly', 'bad', 'not', 'amazing', 'no', 'feel', 'good']\n\nOriginal: This is really good, I absolutely love it!\nProcessed tokens: ['good', 'absolutely', 'love']\n\nOriginal: I hate this, it's not at all enjoyable. Never again.\nProcessed tokens: ['hate', 'not', 'enjoyable', 'never']\n\nYour dataset's vocabulary size after smarter processing with custom stopwords: 39\nWord to index for 'not': 4\nWord to index for 'no': 6\nWord to index for 'is': None\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"small_train_dataset","metadata":{"execution":{"iopub.execute_input":"2025-07-08T17:26:24.472898Z","iopub.status.busy":"2025-07-08T17:26:24.472048Z","iopub.status.idle":"2025-07-08T17:26:24.477082Z","shell.execute_reply":"2025-07-08T17:26:24.476371Z","shell.execute_reply.started":"2025-07-08T17:26:24.472874Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 40000\n","})"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"execution_count":14},{"cell_type":"markdown","source":"###  Build Vocabulary and Determine MAX_LEN from TRAINING SPLIT ONLY ---","metadata":{}},{"cell_type":"code","source":"\ndef batch_tokenize_for_vocab(examples):\n    all_tokens_for_batch_examples = []\n    lengths_in_batch = []\n\n    for text in examples['text']:\n        tokens = custom_tokenize_and_filter(text, nlp, CUSTOM_STOP_WORDS)\n        all_tokens_for_batch_examples.append(tokens)\n        lengths_in_batch.append(len(tokens))\n\n    return {\"_temp_tokens\": all_tokens_for_batch_examples, \"_temp_lengths\": lengths_in_batch}\n\n# It's good practice to set num_proc to the number of CPU cores you want to use.\nNUM_CPU_CORES = os.cpu_count()\nprint(f\"Using {NUM_CPU_CORES} CPU cores for multiprocessing.\")\n\n# Assuming 'small_train_dataset' is already defined.\ntokenized_train_data = small_train_dataset.map(\n    batch_tokenize_for_vocab,\n    batched=True,\n    num_proc=NUM_CPU_CORES,\n    remove_columns=small_train_dataset.column_names\n)\n\n","metadata":{"execution":{"iopub.execute_input":"2025-07-08T17:26:24.477990Z","iopub.status.busy":"2025-07-08T17:26:24.477823Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 4 CPU cores for multiprocessing.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6392b76ca9a646e5819a2fcbccb1e7c9","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=4):   0%|          | 0/40000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":null},{"cell_type":"code","source":"small_train_dataset[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_train_data[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Collect all tokens and lengths from the mapped dataset ---\nall_train_tokens = []\ntrain_lengths = []\n\nfor example in tokenized_train_data:\n\n    all_train_tokens.extend(example['_temp_tokens'])\n\n    # For _temp_lengths:\n    current_lengths = example['_temp_lengths']\n    if isinstance(current_lengths, int):\n        # If it's a single int, wrap it in a list to make it iterable\n        train_lengths.append(current_lengths) # Use append if it's a single item\n    elif isinstance(current_lengths, list):\n        # If it's a list, extend as usual\n        train_lengths.extend(current_lengths)\n    else:\n        # Handle other unexpected types if necessary\n        print(f\"Warning: Unexpected type for _temp_lengths: {type(current_lengths)}\")\n        # Optionally, convert to list and extend, or raise an error\n        train_lengths.extend(list(current_lengths)) # Try to convert to list\n\ntrain_lengths","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### Vocabulary Building \n\ntoken_counts = Counter(all_train_tokens)\n\n# Define special tokens\nPAD_TOKEN = '<PAD>'\nUNK_TOKEN = '<UNK>'\n\n# Build the word_to_idx mapping\nword_to_idx = {PAD_TOKEN: 0, UNK_TOKEN: 1}\ncurrent_idx = 2\nfor word, count in token_counts.most_common(): # Iterates through words by frequency\n    if word not in word_to_idx:\n        word_to_idx[word] = current_idx\n        current_idx += 1\n\nidx_to_word = {idx: word for word, idx in word_to_idx.items()} # Useful for debugging\n\nVOCAB_SIZE = len(word_to_idx)\nprint(f\"Vocabulary size (from train set): {VOCAB_SIZE}\")\nprint(f\"'not' index: {word_to_idx.get('not', 'Not Found')}\")\nprint(f\"'no' index: {word_to_idx.get('no', 'Not Found')}\")\nprint(f\"'is' index: {word_to_idx.get('is', 'Not Found')}\") # Should be 'Not Found'\n\n# Determine MAX_LEN from training data\nMAX_LEN = max(train_lengths)\n# MAX_LEN = int(np.percentile(train_lengths, 99.5))\n\n# Optional: Cap MAX_LEN to a reasonable percentile if you have extreme outliers\n# If MAX_LEN is extremely high due to one very long sentence, it can cause excessive padding.\n# You might do: MAX_LEN = int(np.percentile(train_lengths, 99))\n\n\nprint(f\"Max sequence length (from train set): {MAX_LEN}\")\n\n# --- You would save word_to_idx and MAX_LEN here if this were a production pipeline ---\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Saving word_to_idx and MAX_LEN ---\nimport json\n\noutput_dir = '/kaggle/working'\nos.makedirs(output_dir, exist_ok=True) # Create the directory if it doesn't exist\n\n# Save word_to_idx (as JSON)\nword_to_idx_path = os.path.join(output_dir, 'word_to_idx.json')\nwith open(word_to_idx_path, 'w') as f:\n    json.dump(word_to_idx, f)\nprint(f\"Saved word_to_idx to: {word_to_idx_path}\")\n\n# Save MAX_LEN (as a simple text file)\nmax_len_path = os.path.join(output_dir, 'MAX_LEN.txt')\nwith open(max_len_path, 'w') as f:\n    f.write(str(MAX_LEN))\nprint(f\"Saved MAX_LEN to: {max_len_path}\")\n\n\n\n# Verify\nprint(f\"Is loaded_word_to_idx the same as word_to_idx? {loaded_word_to_idx == word_to_idx}\")\nprint(f\"Is loaded_max_len the same as MAX_LEN? {loaded_max_len == MAX_LEN}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# --- How to load them later ---\nprint(\"\\n--- Demonstrating how to load them later ---\")\n\n\nword_to_idx_path = \"/kaggle/input/word-to-txt/word_to_idx.json\"\n# Load word_to_idx\nwith open(word_to_idx_path, 'r') as f:\n    word_to_idx = json.load(f)\nprint(f\"Loaded word_to_idx. Sample: {list(word_to_idx.items())[:5]}...\")\n\nMAX_LEN = 1337\nVOCAB_SIZE = len(word_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:21:54.090568Z","iopub.execute_input":"2025-07-08T18:21:54.090846Z","iopub.status.idle":"2025-07-08T18:21:54.162251Z","shell.execute_reply.started":"2025-07-08T18:21:54.090826Z","shell.execute_reply":"2025-07-08T18:21:54.161619Z"}},"outputs":[{"name":"stdout","text":"\n--- Demonstrating how to load them later ---\nLoaded word_to_idx. Sample: [('<PAD>', 0), ('<UNK>', 1), ('/><br', 2), ('movie', 3), ('film', 4)]...\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Applying Preprocessing to All Splits\n\nwe'll create a single function that:\n\n    - Takes a batch of examples from the dataset.\n\n    - Applies custom_tokenize_and_filter to each text.\n\n    - Converts tokens to numerical IDs using our word_to_idx.\n\n    - Pads/truncates sequences to MAX_LEN.\n\n    -Generates an attention_mask.","metadata":{}},{"cell_type":"code","source":"NUM_CPU_CORES = os.cpu_count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:25:05.788133Z","iopub.execute_input":"2025-07-08T18:25:05.788857Z","iopub.status.idle":"2025-07-08T18:25:05.792120Z","shell.execute_reply.started":"2025-07-08T18:25:05.788833Z","shell.execute_reply":"2025-07-08T18:25:05.791441Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# --- 3. Create a Preprocessing Function for dataset.map() ---\n# This function will operate on a batch of examples because batched=True is recommended.\n# It will use the globally defined nlp, CUSTOM_STOP_WORDS, word_to_idx, and MAX_LEN.\ndef preprocess_function(examples):\n    # Initialize lists to store processed numerical inputs and masks for the batch\n    input_ids_batch = []\n    attention_mask_batch = []\n\n    # Iterate through each text in the current batch\n    for text in examples['text']:\n        # 1. Tokenize and filter using your custom function\n        processed_tokens = custom_tokenize_and_filter(text, nlp, CUSTOM_STOP_WORDS)\n        \n        # 2. Convert tokens to numerical IDs using the GLOBAL word_to_idx\n        numerical_ids = [word_to_idx.get(token, word_to_idx[UNK_TOKEN]) for token in processed_tokens]\n        \n        # 3. Apply Padding/Truncation using the GLOBAL MAX_LEN\n        if len(numerical_ids) < MAX_LEN:\n            padded_ids = numerical_ids + [word_to_idx[PAD_TOKEN]] * (MAX_LEN - len(numerical_ids))\n            mask = [1] * len(numerical_ids) + [0] * (MAX_LEN - len(numerical_ids))\n        elif len(numerical_ids) > MAX_LEN:\n            padded_ids = numerical_ids[:MAX_LEN]\n            mask = [1] * MAX_LEN\n        else: # Exactly MAX_LEN\n            padded_ids = numerical_ids\n            mask = [1] * MAX_LEN\n\n        input_ids_batch.append(padded_ids)\n        attention_mask_batch.append(mask)\n\n    # Return a dictionary with the new columns. Hugging Face datasets will add these.\n    # Ensure they are lists of lists or NumPy arrays for batched processing.\n    return {\n        'input_ids': input_ids_batch,\n        'attention_mask': attention_mask_batch,\n        # The original 'label' column is kept by default, or you could explicitly include it\n        # 'labels': examples['label']\n    }\n\n\n\n\n\n\n# --- 4. Apply the Preprocessing Function to all splits using .map() ---\nprint(\"\\nApplying preprocessing to dataset splits...\")\n# Remove original 'text' column if you don't need it anymore to save memory\nprocessed_dataset = final_splits_reduced_dataset.map(\n    preprocess_function,\n    batched=True, # Process examples in batches for efficiency\n    num_proc=NUM_CPU_CORES,\n    remove_columns=['text'] # Remove original text column after processing\n)\n\n\n\nprint(processed_dataset)\nprint(f\"Example processed training item: {processed_dataset['train'][0]}\")\nprint(f\"Input IDs shape: {len(processed_dataset['train'][0]['input_ids'])}\")\nprint(f\"Attention Mask shape: {len(processed_dataset['train'][0]['attention_mask'])}\")\n\n\n\n# --- 5. Set the format for PyTorch DataLoaders ---\n# This ensures that when you iterate, you get PyTorch tensors directly.\nprocessed_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\nprint(\"\\nDataset format set to PyTorch:\")\nprint(processed_dataset['train'][0])\n\n# You can now create your PyTorch DataLoaders from these processed datasets:\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:25:26.003871Z","iopub.execute_input":"2025-07-08T18:25:26.004456Z","iopub.status.idle":"2025-07-08T18:39:05.987747Z","shell.execute_reply.started":"2025-07-08T18:25:26.004432Z","shell.execute_reply":"2025-07-08T18:39:05.986900Z"}},"outputs":[{"name":"stdout","text":"\nApplying preprocessing to dataset splits...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef89721ea3441609c3531a4a310cd71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38e0b6eac9204fd294295c1664cda99b"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 40000\n    })\n    validation: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 5000\n    })\n})\nExample processed training item: {'label': 1, 'input_ids': [15622, 18076, 3817, 7, 3104, 657, 1404, 4923, 16, 17, 2, 9121, 2908, 3, 496, 81, 574, 334, 9385, 243, 356, 1600, 26194, 2848, 32005, 10923, 526, 39, 1275, 22577, 40, 14516, 137, 72, 530, 5, 5, 901, 1188, 5, 465, 543, 34895, 1497, 31, 536, 126, 17501, 1188, 34895, 6, 32005, 24823, 40, 3, 150, 956, 78, 10924, 824, 6650, 39, 5512, 611, 3, 450, 2162, 48, 6, 3, 2403, 921, 9385, 653, 1123, 366, 2403, 2162, 1497, 2529, 370, 26195, 2162, 368, 26195, 75, 2162, 62, 396, 3, 25, 948, 26195, 128, 956, 88, 3, 33, 4, 25, 31, 32005, 5, 78, 34896, 12971, 412, 6784, 51501, 312, 4864, 51502, 445, 929, 390, 331, 14516, 423, 33, 40, 16, 17, 2, 2172, 331, 357, 3, 7, 7805, 801, 2546, 78, 50, 26195, 3468, 55, 1880, 2567, 1615, 1395, 1037, 195, 3, 1147, 296, 52, 4864, 23, 363, 22578, 3, 114, 304, 52, 1123, 109, 8362, 2125, 363, 22578, 4593, 23, 3771, 633, 523, 166, 4864, 363, 22578, 2457, 7435, 22578, 898, 5159, 515, 515, 2125, 515, 9, 130, 5840, 75, 1123, 2988, 9, 39, 130, 16, 17, 2, 127, 3, 62, 294, 504, 130, 14516, 296, 301, 1555, 2152, 146, 13848, 130, 215, 6, 2072, 811, 31, 336, 1408, 4, 1756, 6339, 6, 537, 3, 7, 60, 3397, 1399, 383, 383, 1502, 811, 383, 37, 3, 282, 1232, 37, 1854, 383, 541, 925, 1823, 98, 48, 1742, 3609, 181, 24824, 902, 811, 6, 2072, 1321, 2262, 2291, 1232, 776, 238, 37, 356, 5513, 16, 17, 2, 56, 244, 172, 20, 14516, 301, 4, 49, 147, 796, 50, 2162, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\nInput IDs shape: 1337\nAttention Mask shape: 1337\n\nDataset format set to PyTorch:\n{'label': tensor(1), 'input_ids': tensor([15622, 18076,  3817,  ...,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0])}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"\nsave_directory = \"./my_processed_dataset\"\nprocessed_dataset.save_to_disk(save_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:42:18.158073Z","iopub.execute_input":"2025-07-08T18:42:18.158763Z","iopub.status.idle":"2025-07-08T18:42:18.492477Z","shell.execute_reply.started":"2025-07-08T18:42:18.158733Z","shell.execute_reply":"2025-07-08T18:42:18.491876Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6d0688083e4673a31984bc58a02810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74a8aed6bb104d87bfb0f7f6f6b2a807"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"717c55ca6adb4664b1d8672f9e8fdce4"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from IPython.display import FileLink\n\ndataset_folder_path = \"/kaggle/working/my_processed_dataset\"\n\n# Define the name for your zip file (this will be created in /kaggle/working/)\nzip_file_name = \"my_processed_dataset.zip\"\n\n# Ensure you are in the /kaggle/working/ directory before zipping\n# This makes it easier for the zip command to find the folder.\nos.chdir('/kaggle/working/')\n\n# Use the 'zip' command to compress the folder\n# -r: recursively include directories and their contents (including subfolders)\n# The command will create 'my_processed_dataset.zip' containing the\n# 'my_processed_dataset' folder and all its contents.\n!zip -r {zip_file_name} {dataset_folder_path}\n\n# Generate a downloadable link\nFileLink(zip_file_name)\n\nprint(f\"\\nYour dataset folder has been zipped to '{zip_file_name}'. Click the link above to download it to your local machine.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:42:33.140615Z","iopub.execute_input":"2025-07-08T18:42:33.141146Z","iopub.status.idle":"2025-07-08T18:42:37.272864Z","shell.execute_reply.started":"2025-07-08T18:42:33.141124Z","shell.execute_reply":"2025-07-08T18:42:37.272137Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/my_processed_dataset/ (stored 0%)\n  adding: kaggle/working/my_processed_dataset/dataset_dict.json (deflated 5%)\n  adding: kaggle/working/my_processed_dataset/test/ (stored 0%)\n  adding: kaggle/working/my_processed_dataset/test/dataset_info.json (deflated 70%)\n  adding: kaggle/working/my_processed_dataset/test/data-00000-of-00001.arrow (deflated 96%)\n  adding: kaggle/working/my_processed_dataset/test/state.json (deflated 39%)\n  adding: kaggle/working/my_processed_dataset/train/ (stored 0%)\n  adding: kaggle/working/my_processed_dataset/train/dataset_info.json (deflated 70%)\n  adding: kaggle/working/my_processed_dataset/train/data-00000-of-00001.arrow (deflated 96%)\n  adding: kaggle/working/my_processed_dataset/train/state.json (deflated 39%)\n  adding: kaggle/working/my_processed_dataset/validation/ (stored 0%)\n  adding: kaggle/working/my_processed_dataset/validation/dataset_info.json (deflated 70%)\n  adding: kaggle/working/my_processed_dataset/validation/data-00000-of-00001.arrow (deflated 96%)\n  adding: kaggle/working/my_processed_dataset/validation/state.json (deflated 39%)\n\nYour dataset folder has been zipped to 'my_processed_dataset.zip'. Click the link above to download it to your local machine.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Creating DataLoaders","metadata":{}},{"cell_type":"code","source":"## Important Hyperparameter\nBATCH_SIZE = 32\n\ndef custom_collate_fn(batch):\n    # 'batch' is a list of dictionaries, one for each sample in the batch\n    # e.g., [{'input_ids': tensor, 'attention_mask': tensor, 'label': tensor}, ...]\n\n    input_ids = torch.stack([item['input_ids'].to(torch.long) for item in batch])\n    attention_mask = torch.stack([item['attention_mask'].to(torch.long) for item in batch])\n    labels = torch.tensor([item['label'] for item in batch], dtype=torch.float32)\n\n    # You can return a tuple or a dictionary\n    # Returning a tuple matches your original loop expectation (if you only want 2 items)\n    return (input_ids, attention_mask, labels) # This would still be 3 items\n\n\n\n\n### Creating DataLoader\ntrain_dataloader = DataLoader(processed_dataset['train'], batch_size=BATCH_SIZE,collate_fn = custom_collate_fn, shuffle=True,num_workers = 4)\nval_dataloader = DataLoader(processed_dataset['validation'],collate_fn = custom_collate_fn, batch_size=BATCH_SIZE,num_workers =4)\ntest_dataloader = DataLoader(processed_dataset['test'],collate_fn = custom_collate_fn, batch_size=BATCH_SIZE,num_workers = 4)\n\n\nprint(f\"\\nFirst batch from DataLoader (example):\")\nfor batch_input_ids,batch_label,batch_attention_mask in train_dataloader:\n    \n    print(f\"Input IDs batch shape: {type(batch_input_ids.shape)}\")\n    print(f\"Attention Mask batch shape: {batch_attention_mask.shape}\")\n    print(f\"Labels batch shape: {batch_label.shape}\")\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:45:27.110188Z","iopub.execute_input":"2025-07-08T18:45:27.110520Z","iopub.status.idle":"2025-07-08T18:45:27.293605Z","shell.execute_reply.started":"2025-07-08T18:45:27.110491Z","shell.execute_reply":"2025-07-08T18:45:27.292592Z"}},"outputs":[{"name":"stdout","text":"\nFirst batch from DataLoader (example):\nInput IDs batch shape: <class 'torch.Size'>\nAttention Mask batch shape: torch.Size([32])\nLabels batch shape: torch.Size([32, 1337])\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Loading Glove Embeddings and Creating the Embedded Matrix for the entire pipeline","metadata":{}},{"cell_type":"code","source":"\nGLOVE_PATH = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt' # Make sure this file is in the same directory or provide full path\nGLOVE_EMBEDDING_DIM = 100 # Match the dimension of your downloaded GloVe file\n\nglove_embeddings = {}\ntry:\n    with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            glove_embeddings[word] = vector\n    print(f\"Loaded {len(glove_embeddings)} words from GloVe.\")\nexcept FileNotFoundError:\n    print(f\"Error: GloVe file not found at {GLOVE_PATH}. Please download it from https://nlp.stanford.edu/projects/glove/.\")\n    print(\"For example, download glove.6B.zip and extract glove.6B.100d.txt.\")\n    exit() # Exit if GloVe not found, as it's critical for this part\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:19.757662Z","iopub.execute_input":"2025-07-08T18:46:19.758160Z","iopub.status.idle":"2025-07-08T18:46:29.159093Z","shell.execute_reply.started":"2025-07-08T18:46:19.758138Z","shell.execute_reply":"2025-07-08T18:46:29.158403Z"}},"outputs":[{"name":"stdout","text":"Loaded 400000 words from GloVe.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# --- Step 3: Create the Embedding Matrix for Your Model ---\n# Initialize with random values (or zeros) for words not found in GloVe\n# Ensure the matrix has the correct dimensions: (your_vocab_size, embedding_dim)\nembedding_matrix = np.random.uniform(-0.25, 0.25, (VOCAB_SIZE, GLOVE_EMBEDDING_DIM))\n\n# Fill the matrix with GloVe vectors where available\nfor word, idx in word_to_idx.items():\n    if word in glove_embeddings:\n        embedding_matrix[idx] = glove_embeddings[word]\n    else:\n        # Handle Out-Of-Vocabulary (OOV) words from your dataset that are not in GloVe\n        # For UNK_TOKEN, it will be randomly initialized, which is fine.\n        # For PAD_TOKEN, it remains randomly initialized, or you could explicitly set it to zeros.\n        if word == PAD_TOKEN:\n            embedding_matrix[idx] = np.zeros(GLOVE_EMBEDDING_DIM) # Set PAD to zeros\n        # Other OOV words in your vocab will remain randomly initialized.\n\n# Convert to a PyTorch tensor\nembedding_matrix_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n\nprint(f\"\\nShape of the final embedding matrix for your model: {embedding_matrix_tensor.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:29.160178Z","iopub.execute_input":"2025-07-08T18:46:29.160443Z","iopub.status.idle":"2025-07-08T18:46:29.468167Z","shell.execute_reply.started":"2025-07-08T18:46:29.160419Z","shell.execute_reply":"2025-07-08T18:46:29.467328Z"}},"outputs":[{"name":"stdout","text":"\nShape of the final embedding matrix for your model: torch.Size([128465, 100])\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# --- Step 4: Integrate into your VanillaRNN model ---\n\nclass VanillaRNNWithPretrainedEmbeddings(nn.Module):\n    def __init__(self, embedding_matrix, hidden_dim=100, output_dim=1, freeze_embeddings=True):\n        super().__init__()\n        \n        # Create the embedding layer using nn.Embedding.from_pretrained\n        # It automatically sets num_embeddings and embedding_dim from the matrix shape\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n        \n        self.rnn = nn.RNN(self.embedding.embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, input_ids, attention_mask):\n        embeddings = self.embedding(input_ids)\n        \n        lengths = attention_mask.sum(dim=1).cpu() # .cpu() might be needed for pack_padded_sequence older versions\n        lengths = torch.max(lengths, torch.tensor(1, device=lengths.device))\n\n        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n            embeddings, lengths, batch_first=True, enforce_sorted=False\n        )\n\n        packed_hidden_states, final_hidden_state = self.rnn(packed_embeddings)\n\n        final_state_squeezed = final_hidden_state.squeeze(0)\n        output = self.fc(final_state_squeezed)\n\n        return output\n\n# --- Instantiate and Check Summary ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate the model with your prepared embedding matrix\n# Set freeze_embeddings=True to keep them static (recommended for small datasets)\nmodel_pretrained_emb = VanillaRNNWithPretrainedEmbeddings(\n    embedding_matrix=embedding_matrix_tensor,\n    freeze_embeddings=True # This is crucial!\n).to(device)\n\nprint(\"\\n--- Model Parameters with Pre-trained Embeddings ---\")\ntotal_params = sum(p.numel() for p in model_pretrained_emb.parameters())\ntrainable_params = sum(p.numel() for p in model_pretrained_emb.parameters() if p.requires_grad)\nnon_trainable_params = sum(p.numel() for p in model_pretrained_emb.parameters() if not p.requires_grad)\n\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Trainable parameters: {trainable_params}\")\nprint(f\"Non-trainable parameters: {non_trainable_params}\")\n\n# Prepare dummy inputs for torchinfo summary\nbatch_size = 32 # Can be larger than 1 since embeddings are smaller\nMAX_LEN = 50 # Example max sequence length\ndummy_input_ids = torch.randint(0, VOCAB_SIZE, (batch_size, MAX_LEN), dtype=torch.long).to(device)\ndummy_attention_mask = torch.ones((batch_size, MAX_LEN), dtype=torch.long).to(device)\nfor i in range(batch_size):\n    rand_len = torch.randint(low=1, high=MAX_LEN + 1, size=(1,)).item()\n    dummy_attention_mask[i, rand_len:] = 0\n\nfrom torchinfo import summary\nprint(\"\\n--- Model Summary (VanillaRNNWithPretrainedEmbeddings) ---\")\nsummary(model_pretrained_emb, input_data=(dummy_input_ids, dummy_attention_mask), device=str(device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:29.469141Z","iopub.execute_input":"2025-07-08T18:46:29.469406Z","iopub.status.idle":"2025-07-08T18:46:30.081494Z","shell.execute_reply.started":"2025-07-08T18:46:29.469357Z","shell.execute_reply":"2025-07-08T18:46:30.080733Z"}},"outputs":[{"name":"stdout","text":"\n--- Model Parameters with Pre-trained Embeddings ---\nTotal parameters: 12866801\nTrainable parameters: 20301\nNon-trainable parameters: 12846500\n\n--- Model Summary (VanillaRNNWithPretrainedEmbeddings) ---\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaRNNWithPretrainedEmbeddings       [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                               [751, 100]                20,200\n├─Linear: 1-3                            [32, 1]                   101\n==========================================================================================\nTotal params: 12,866,801\nTrainable params: 20,301\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 1.93\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 1.88\nParams size (MB): 51.47\nEstimated Total Size (MB): 53.37\n=========================================================================================="},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"HIDDEN_DIM = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.083022Z","iopub.execute_input":"2025-07-08T18:46:30.083271Z","iopub.status.idle":"2025-07-08T18:46:30.086461Z","shell.execute_reply.started":"2025-07-08T18:46:30.083255Z","shell.execute_reply":"2025-07-08T18:46:30.085770Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# Attention Classes\n","metadata":{}},{"cell_type":"code","source":"\n## Assuming hidden_dim is the size of the hidden state of the RNN/encoder\nclass BahdanauAttention(nn.Module):\n    def __init__(self, decoder_hidden_dim, encoder_hidden_dim,attention_hidden_dim):\n        super(BahdanauAttention, self).__init__()\n        # W_1 (also sometimes called W_query or W_decoder)\n        self.W1 = nn.Linear(decoder_hidden_dim,attention_hidden_dim, bias=False) ## d_decoder * d_attention\n        # W_2 (also sometimes called W_key or W_encoder)\n        self.W2 = nn.Linear(encoder_hidden_dim, attention_hidden_dim, bias=False) ## d_encoder * d_attention\n        # v_a (also sometimes called V or W_score)\n        self.V = nn.Linear(attention_hidden_dim, 1, bias=False)   ## d_attention * 1     \n\n    def forward(self, query_state, encoder_hidden_states, attention_mask=None):\n        # query_state: (batch_size, hidden_dim) - this is typically the decoder's current hidden state\n        # encoder_hidden_states: (batch_size, seq_len, hidden_dim) - these are the outputs from the encoder\n\n        # Step 1 & 2: Transform the query and keys\n        # transformed_query: (batch_size, hidden_dim)\n        transformed_query = self.W1(query_state)\n        # transformed_encoder_states: (batch_size, seq_len, hidden_dim)\n        transformed_encoder_states = self.W2(encoder_hidden_states)\n\n        # Step 3: Combine (Add) and Apply Non-Linearity\n        # We need to unsqueeze transformed_query to (batch_size, 1, hidden_dim)\n        # so it can be broadcasted and added to transformed_encoder_states.\n        # energies: (batch_size, seq_len, hidden_dim)\n        energies = torch.tanh(transformed_query.unsqueeze(1) + transformed_encoder_states)\n\n        # Step 4: Calculate Raw Attention Scores (Energies)\n        # attention_scores: (batch_size, seq_len, 1)\n        attention_scores = self.V(energies).squeeze(2) # squeeze to (batch_size, seq_len)\n\n        # Apply attention mask if provided (important for padded sequences!)\n        if attention_mask is not None:\n            # Mask out padded positions. Fill with a very small negative number\n            # so that after softmax, they become virtually 0.\n            attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e9)\n\n        # Step 5: Compute Attention Weights\n        # attention_weights: (batch_size, seq_len) - sum to 1 across seq_len dim\n        attention_weights = F.softmax(attention_scores, dim=1)\n\n        # Step 6: Calculate Context Vector\n       \n        # context_vector: (batch_size, hidden_dim)\n        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_hidden_states).squeeze(1)\n        \n        return context_vector, attention_weights\n    \n\n\n\n\nclass LuongDotProductAttention(nn.Module):\n    def __init__(self,decoder_hidden_dim, encoder_hidden_dim):\n        super(LuongDotProductAttention, self).__init__()\n\n        if decoder_hidden_dim != encoder_hidden_dim:\n            raise ValueError(\"Decoder hidden dimension must match encoder hidden dimension for Luong dot product attention.\")\n        # W_1 (also sometimes called W_query or W_decoder)\n\n    def forward(self,query_state, encoder_hidden_states,attention_mask = None):\n\n        # (B,D) -> (B,D,1)\n        # (B,L,D) @ (B,D,1) -> (B,L,1)\n\n        attention_scores = torch.bmm(encoder_hidden_states,query_state.unsqueeze(2)).squeeze(2)\n\n        if attention_mask is not None:\n            # Mask out padded positions. Fill with a very small negative number\n            # so that after softmax, they become virtually 0.\n            attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e9)\n\n        attention_weights = F.softmax(attention_scores, dim=1)\n\n\n        #  (B,L) => (B,1,L) @ (B,L,D) => (B,1,D) =>(B,D)\n\n        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_hidden_states).squeeze(1)\n       \n\n        return context_vector, attention_weights\n    \n\n\nclass LuongGeneralAttention(nn.Module):\n    def __init__(self,decoder_hidden_dim, encoder_hidden_dim):\n        super(LuongGeneralAttention, self).__init__()\n\n     \n        # W_a\n        self.W_a = nn.Linear(decoder_hidden_dim,encoder_hidden_dim ,bias=False)  ### d_h  -->d_e \n   \n\n\n    def forward(self,query_state, encoder_hidden_states,attention_mask = None):\n\n        # (B,D) -> (B,D,1)\n        # (B,L,D) @ (B,D,1) -> (B,L,1)\n        query_state_transformed = self.W_a(query_state)\n\n        attention_scores = torch.bmm(encoder_hidden_states,query_state_transformed.unsqueeze(2)).squeeze(2)\n\n\n\n\n\n\n        if attention_mask is not None:\n            # Mask out padded positions. Fill with a very small negative number\n            # so that after softmax, they become virtually 0.\n            attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e9)\n\n        attention_weights = F.softmax(attention_scores, dim=1)\n\n\n        #  (B,L) => (B,1,L) @ (B,L,D) => (B,1,D) =>(B,D)\n\n        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_hidden_states).squeeze(1)\n       \n\n        return context_vector, attention_weights\n    \n\n\nclass LuongConcatAttention(nn.Module):\n    def __init__(self,decoder_hidden_dim, encoder_hidden_dim,attention_hidden_dim):\n        super(LuongConcatAttention, self).__init__()\n\n     \n        # W_c\n        self.W_c = nn.Linear(decoder_hidden_dim + encoder_hidden_dim,attention_hidden_dim, bias=False)\n\n        self.V = nn.Linear(attention_hidden_dim, 1, bias=False)   ## d_attention * 1    \n\n\n    def forward(self,query_state, encoder_hidden_states,attention_mask = None):\n\n        # (B,D_d) => (B,1,D_d) => (B,L,D_d)\n        transformed_query = query_state.unsqueeze(1).expand(-1, encoder_hidden_states.size(1), -1)  # (B, L, D_d)\n\n\n\n\n\n        concat_query_encoder_hidden_states = torch.cat((transformed_query,encoder_hidden_states),dim=2)  #(B, L, D_d + D_e)\n\n\n\n        energies = torch.tanh(self.W_c(concat_query_encoder_hidden_states))#(B,L,D_a)\n\n\n    \n        # attention_scores: (batch_size, seq_len, 1)\n        attention_scores = self.V(energies).squeeze(2) # squeeze to (batch_size, seq_len)\n\n        if attention_mask is not None:\n            # Mask out padded positions. Fill with a very small negative number\n            # so that after softmax, they become virtually 0.\n            attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e9)\n\n        attention_weights = F.softmax(attention_scores, dim=1)\n\n\n        #  (B,L) => (B,1,L) @ (B,L,D) => (B,1,D) =>(B,D)\n\n        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_hidden_states).squeeze(1)\n       \n\n        return context_vector, attention_weights","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.087189Z","iopub.execute_input":"2025-07-08T18:46:30.087431Z","iopub.status.idle":"2025-07-08T18:46:30.106240Z","shell.execute_reply.started":"2025-07-08T18:46:30.087408Z","shell.execute_reply":"2025-07-08T18:46:30.105671Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# Vanilla Models","metadata":{}},{"cell_type":"code","source":"class VanillaRNN(nn.Module):\n    def __init__(self,embedding_matrix, hidden_dim=HIDDEN_DIM, output_dim=1, freeze_embeddings=True):\n        super().__init__()\n         # Create the embedding layer using nn.Embedding.from_pretrained\n        # It automatically sets num_embeddings and embedding_dim from the matrix shape\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n        self.rnn = nn.RNN(self.embedding.embedding_dim, hidden_dim, batch_first=True) # batch_first=True is important\n        self.fc = nn.Linear(hidden_dim, output_dim) # Output dimension for binary classification\n\n    def forward(self, input_ids, attention_mask):\n        # 1. Embed the input_ids\n        # input_ids shape: (batch_size, sequence_length)\n        # embeddings shape: (batch_size, sequence_length, embedding_dim)\n        embeddings = self.embedding(input_ids)\n\n        # 2. Get original (non-padded) sequence lengths from attention_mask\n        # For each sequence in the batch, sum the '1's in its attention_mask.\n        # Ensure lengths are on CPU for pack_padded_sequence if your data is on GPU.\n        # This will be a 1D tensor of shape (batch_size,)\n        lengths = attention_mask.sum(dim=1).cpu()\n\n        # Handle potential zero-length sequences (though unlikely in sentiment analysis)\n        # pack_padded_sequence requires lengths to be > 0.\n        # If any length is zero, set it to 1 to avoid error, and mask its contribution later if needed.\n        # For typical tokenization, min length will be 2 ([CLS], [SEP]).\n        # If there's a risk of 0 lengths, you might replace them with 1 and then zero out their contribution to loss.\n        # For most practical cases with a tokenizer adding special tokens, this isn't strictly necessary\n        # but good to be aware of.\n        lengths = torch.max(lengths, torch.tensor(1, device=lengths.device))\n\n        # 3. Pack the padded sequence\n        # This prevents the RNN from processing padding tokens.\n        # It creates a PackedSequence object that the RNN understands.\n        # embeddings shape: (batch_size, sequence_length, embedding_dim)\n        # lengths shape: (batch_size,)\n        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n            embeddings, lengths, batch_first=True, enforce_sorted=False # enforce_sorted=False if your lengths aren't sorted\n        )\n\n        # 4. Pass the packed sequence through the RNN\n        # hidden_states: PackedSequence object containing output for non-padded tokens\n        # final_hidden_state: Tensor containing the last hidden state for each sequence (h_n, c_n for LSTM)\n        # Note: for RNN, final_hidden_state is (num_layers * num_directions, batch_size, hidden_size)\n        # For a simple, non-bidirectional RNN with 1 layer, this is (1, batch_size, hidden_size)\n        _, final_hidden_state = self.rnn(packed_embeddings)\n\n        # 5. Unpack the hidden states (optional, but useful if you plan to use attention on all hidden states)\n        # This converts the PackedSequence back to a padded tensor with original batch_first structure.\n        # padded_hidden_states, _ = nn.utils.rnn.pad_packed_sequence(\n        #     packed_hidden_states, batch_first=True, total_length=input_ids.shape[1]\n        # )\n        # padded_hidden_states shape: (batch_size, sequence_length, hidden_dim)\n\n        # 6. Use the final hidden state for classification\n        # For a simple RNN, the final_hidden_state (h_n) represents the context of the sequence.\n        # Since batch_first=True in the RNN, final_hidden_state is (1, batch_size, hidden_dim).\n        # We need to remove the first dimension (num_layers * num_directions) to get (batch_size, hidden_dim).\n        # If it were bidirectional, final_hidden_state would be (2, batch_size, hidden_dim),\n        # requiring concatenation of forward/backward states before the fc layer.\n        final_state_squeezed = final_hidden_state.squeeze(0) # shape: (batch_size, hidden_dim)\n\n        # 7. Pass through the final linear layer\n        # output will be logits, shape: (batch_size, output_dim)\n        output = self.fc(final_state_squeezed)\n\n        return output\n\n\n\nclass VanillaLSTM(nn.Module):\n    def __init__(self,embedding_matrix, hidden_dim=HIDDEN_DIM, output_dim=1,freeze_embeddings=True):\n        super().__init__()\n        self.embedding =  nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n        self.lstm = nn.LSTM(self.embedding.embedding_dim, hidden_dim, batch_first=True) # batch_first=True is important\n        self.fc = nn.Linear(hidden_dim, output_dim) # Output dimension for binary classification\n\n    def forward(self, input_ids, attention_mask):\n        # 1. Embed the input_ids\n        embeddings = self.embedding(input_ids)\n\n        # 2. Get original (non-padded) sequence lengths from attention_mask\n        lengths = attention_mask.sum(dim=1).cpu()\n\n        # 3. Pack the padded sequence\n        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n            embeddings, lengths, batch_first=True, enforce_sorted=False\n        )\n\n        # 4. Pass the packed sequence through the LSTM\n        _, (final_hidden_state, _) = self.lstm(packed_embeddings)\n\n        # 5. Use the final hidden state for classification\n        final_state_squeezed = final_hidden_state.squeeze(0) # shape: (batch_size, hidden_dim)\n\n        # 6. Pass through the final linear layer\n        output = self.fc(final_state_squeezed)\n\n        return output\n    \nclass VanillaBidirectionalRNN(nn.Module):\n    def __init__(self, embedding_matrix, hidden_dim=HIDDEN_DIM, output_dim=1,freeze_embeddings=True):\n        super().__init__()\n        self.embedding =  nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n        self.rnn = nn.RNN(self.embedding.embedding_dim, hidden_dim, batch_first=True, bidirectional=True) # bidirectional=True\n        self.fc = nn.Linear(hidden_dim * 2, output_dim) # Output dimension for binary classification\n\n    def forward(self, input_ids, attention_mask):\n        # 1. Embed the input_ids\n        embeddings = self.embedding(input_ids)\n\n        # 2. Get original (non-padded) sequence lengths from attention_mask\n        lengths = attention_mask.sum(dim=1).cpu()\n\n        # 3. Pack the padded sequence\n        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n            embeddings, lengths, batch_first=True, enforce_sorted=False\n        )\n\n        # 4. Pass the packed sequence through the RNN\n        _, final_hidden_state = self.rnn(packed_embeddings)\n\n        # 5. Use the final hidden state for classification\n        # For bidirectional RNNs, final_hidden_state will have shape (2, batch_size, hidden_dim)\n        final_state_squeezed = final_hidden_state.view(final_hidden_state.size(1), -1) # shape: (batch_size, hidden_dim * 2)\n\n        # 6. Pass through the final linear layer\n        output = self.fc(final_state_squeezed)  \n\n\n\n\n        return output\n    \n\n\nclass VanillaBidirectionalLSTM(nn.Module):\n    def __init__(self, embedding_matrix, hidden_dim=HIDDEN_DIM, output_dim=1, freeze_embeddings=True):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze_embeddings)  # Freeze embeddings by default\n        self.lstm = nn.LSTM(self.embedding.embedding_dim, hidden_dim, batch_first=True, bidirectional=True) # bidirectional=True\n        self.fc = nn.Linear(hidden_dim * 2, output_dim) # Output dimension for binary classification\n\n    def forward(self, input_ids, attention_mask):\n        # 1. Embed the input_ids\n        embeddings = self.embedding(input_ids)\n\n        # 2. Get original (non-padded) sequence lengths from attention_mask\n        lengths = attention_mask.sum(dim=1).cpu()\n\n        # 3. Pack the padded sequence\n        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n            embeddings, lengths, batch_first=True, enforce_sorted=False\n        )\n\n        # 4. Pass the packed sequence through the LSTM\n        _, (final_hidden_state, _) = self.lstm(packed_embeddings)\n\n        # 5. Use the final hidden state for classification\n        final_state_squeezed = final_hidden_state.view(final_hidden_state.size(1), -1) # shape: (batch_size, hidden_dim * 2)\n\n        # 6. Pass through the final linear layer\n        output = self.fc(final_state_squeezed)                              \n\n        return output\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.106924Z","iopub.execute_input":"2025-07-08T18:46:30.107182Z","iopub.status.idle":"2025-07-08T18:46:30.122480Z","shell.execute_reply.started":"2025-07-08T18:46:30.107162Z","shell.execute_reply":"2025-07-08T18:46:30.121944Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"# General Purpose Models with Plug and Play Attention","metadata":{}},{"cell_type":"code","source":"\n\n# --- General Purpose RNN Model ---\nclass VanillaRNNWithPlugAndPlayAttention(nn.Module):\n    def __init__(self,embedding_matrix, attention_class, attention_kwargs,return_attention_weights =False,\n                  hidden_dim=HIDDEN_DIM, output_dim=1,freeze_embeddings=True):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix,freeze= freeze_embeddings)\n           # We need to ensure the RNN outputs all hidden states. batch_first=True is important.\n        self.rnn = nn.RNN(self.embedding.embedding_dim, hidden_dim, batch_first=True)\n        \n        # Instantiate the attention module using the provided class and kwargs\n        # The **attention_kwargs unpacks the dictionary into named arguments\n        self.attention = attention_class(\n            decoder_hidden_dim=hidden_dim, # Decoder's hidden state is the query for attention\n            encoder_hidden_dim=hidden_dim, # Encoder's outputs are the keys/values for attention\n            **attention_kwargs # This will pass specific kwargs like 'attention_dim' if present\n        )\n        self.return_attention_weights = return_attention_weights\n  \n\n\n        # The final fully connected layer will now take the concatenated \n        # final hidden state AND the context vector from attention.\n        # So, its input dimension becomes hidden_dim + hidden_dim = 2 * hidden_dim\n\n        # Assuming context_vector will also be of hidden_dim size (typical)   (! I don't find where it isn't the case!)\n\n        self.fc = nn.Linear(hidden_dim * 2, output_dim) \n\n    def forward(self, input_ids, attention_mask):\n                # 1. Embed the input_ids\n        embeddings = self.embedding(input_ids)\n\n                # 2. Get original (non-padded) sequence lengths from attention_mask\n        lengths = attention_mask.sum(dim=1).cpu()\n   # 3. Pack the padded sequence\n        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n            embeddings, lengths, batch_first=True, enforce_sorted=False\n        )\n     \n        # 4. Pass the packed sequence through the RNN\n        packed_output, final_hidden_state = self.rnn(packed_embeddings)\n\n      # 5. Unpack the RNN outputs to get all encoder_hidden_states\n\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n            packed_output, batch_first=True, total_length=input_ids.size(1)\n        )\n          # 6 Squeeze the final_hidden_state to (batch_size, hidden_dim) to use as query\n        query = final_hidden_state.squeeze(0) \n\n        # Calculate context vector using the plug-and-play attention module\n        context_vector, attention_weights = self.attention(\n            query_state=query, \n            encoder_hidden_states=outputs, \n            attention_mask=attention_mask\n        )\n # 7. Concatenate the query (final hidden state) with the context vector\n        # This combines the \"sequential summary\" with the \"attention-weighted summary\"\n        combined_representation = torch.cat((query, context_vector), dim=1)\n        output = self.fc(combined_representation)  \n\n        if self.return_attention_weights:\n            # If we want to return attention weights for visualization/debugging\n            return output, attention_weights\n        else:\n            return output \n\n\n\n\nclass VanillaBidirectionalRNNWithPlugAndPlayAttention(nn.Module):\n    def __init__(self,embedding_matrix, attention_class, attention_kwargs,return_attention_weights =False,\n                 hidden_dim=HIDDEN_DIM, output_dim=1,freeze_embeddings=True):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix,freeze= freeze_embeddings)\n        self.rnn = nn.RNN(self.embedding.embedding_dim, hidden_dim, batch_first=True, bidirectional=True) # bidirectional=True\n        self.fc = nn.Linear(hidden_dim * 4, output_dim) # Output dimension for binary classification\n\n\n        self.attention = attention_class(\n            decoder_hidden_dim=2*hidden_dim, # Decoder's hidden state is the query for attention\n            encoder_hidden_dim=2*hidden_dim, # Encoder's outputs are the keys/values for attention\n            **attention_kwargs # This will pass specific kwargs like 'attention_dim' if present\n        )\n        self.return_attention_weights = return_attention_weights\n\n\n    def forward(self, input_ids, attention_mask):\n        # 1. Embed the input_ids\n        embeddings = self.embedding(input_ids) #(B,L,E)\n\n        # 2. Get original (non-padded) sequence lengths from attention_mask\n        lengths = attention_mask.sum(dim=1).cpu()\n\n        # 3. Pack the padded sequence\n        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n            embeddings, lengths, batch_first=True, enforce_sorted=False\n        )\n\n        # 4. Pass the packed sequence through the RNN\n        # packed_hidden_states: PackedSequence of (B, L, 2*H)\n        # final_hidden_state: (num_layers * num_directions, B, H) -> (2, B, H) for 1-layer bidirectional\n        packed_hidden_states, final_hidden_state = self.rnn(packed_embeddings)\n            \n        # 5. Unpack the rnn outputs to get all encoder_hidden_states\n        # outputs: (B, L, 2*H) - this contains the concatenated forward and backward hidden states for each time step\n\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n            packed_hidden_states, batch_first=True, total_length=input_ids.size(1)\n        )\n        # For bidirectional RNNs, final_hidden_state will have shape (2, batch_size, hidden_dim)\n        #6 Squeeze the final_hidden_state  (2,batch_size,hidden_dim) to (batch_size, 2*hidden_dim) to use as query\n        query = final_hidden_state.view(final_hidden_state.size(1), -1) # shape: (batch_size, hidden_dim * 2)\n        #7Calculate context vector using the plug-and-play attention module\n        context_vector, attention_weights = self.attention(\n            query_state=query, \n            encoder_hidden_states=outputs, \n            attention_mask=attention_mask\n        )\n\n    \n       \n\n\n        # 8. Concatenate the query (final hidden state) with the context vector\n        # query: (B, 2*H)\n        # context_vector: (B, 2*H) - typically, attention outputs context vector of encoder_hidden_dim\n        # combined_representation: (B, 4*H)\n        # This combines the \"sequential summary\" with the \"attention-weighted summary\"\n\n\n        combined_representation = torch.cat((query, context_vector), dim=1)    #(B,2*H)  +(B,2*hidden_dim) = (B,4*hidden_dim)\n        \n\n      \n        # 9. Pass through the final linear layer\n        output = self.fc(combined_representation)  \n\n\n\n # Return attention weights for visualization/debugging\n  \n        if self.return_attention_weights:\n        \n            return output, attention_weights\n        else:\n            return output  # Only return the output if weights are not explicitly requested\n\n\n\n# --- General Purpose LSTM Model ---\nclass VanillaLSTMWithPlugAndPlayAttention(nn.Module):\n    def __init__(self,embedding_matrix, attention_class, attention_kwargs,return_attention_weights =False,\n                 hidden_dim=HIDDEN_DIM, output_dim=1,freeze_embeddings=True):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix,freeze= freeze_embeddings)\n           # We need to ensure the RNN outputs all hidden states. batch_first=True is important.\n        self.lstm = nn.LSTM(self.embedding.embedding_dim, hidden_dim, batch_first=True)\n        \n        # Instantiate the attention module using the provided class and kwargs\n        # The **attention_kwargs unpacks the dictionary into named arguments\n        self.attention = attention_class(\n            decoder_hidden_dim=hidden_dim, # Decoder's hidden state is the query for attention\n            encoder_hidden_dim=hidden_dim, # Encoder's outputs are the keys/values for attention\n            **attention_kwargs # This will pass specific kwargs like 'attention_dim' if present\n        )\n        self.return_attention_weights = return_attention_weights\n  \n\n\n        # The final fully connected layer will now take the concatenated \n        # final hidden state AND the context vector from attention.\n        # So, its input dimension becomes hidden_dim + hidden_dim = 2 * hidden_dim\n\n        # Assuming context_vector will also be of hidden_dim size (typical)   (! I don't find where it isn't the case!)\n\n        self.fc = nn.Linear(hidden_dim * 2, output_dim) \n\n    def forward(self, input_ids, attention_mask):\n                # 1. Embed the input_ids\n        embeddings = self.embedding(input_ids)\n\n                # 2. Get original (non-padded) sequence lengths from attention_mask\n        lengths = attention_mask.sum(dim=1).cpu()\n   # 3. Pack the padded sequence\n        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n            embeddings, lengths, batch_first=True, enforce_sorted=False\n        )\n     \n        # 4. Pass the packed sequence through the lstm\n        packed_output, (final_hidden_state, _)  = self.lstm(packed_embeddings)\n\n      # 5. Unpack the LSTM outputs to get all encoder_hidden_states\n\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n            packed_output, batch_first=True, total_length=input_ids.size(1)\n        )\n          # 6 Squeeze the final_hidden_state to (batch_size, hidden_dim) to use as query\n        query = final_hidden_state.squeeze(0) \n\n        # Calculate context vector using the plug-and-play attention module\n        context_vector, attention_weights = self.attention(\n            query_state=query, \n            encoder_hidden_states=outputs, \n            attention_mask=attention_mask\n        )\n # 7. Concatenate the query (final hidden state) with the context vector\n        # This combines the \"sequential summary\" with the \"attention-weighted summary\"\n        combined_representation = torch.cat((query, context_vector), dim=1)\n        output = self.fc(combined_representation)  \n\n\n        # Conditionally return attention weights on basis of flags\n\n        # Allows for visualization or debugging of attention mechanisms\n        if self.return_attention_weights:\n            return output, attention_weights\n        else:\n            return output \n\n\n\n## General Purpose Bidirectional LSTM Model\nclass VanillaBidirectionalLSTMWithPlugAndPlayAttention(nn.Module):\n    def __init__(self,embedding_matrix, attention_class, attention_kwargs,return_attention_weights =False,\n                 hidden_dim=HIDDEN_DIM, output_dim=1,freeze_embeddings=True):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix,freeze= freeze_embeddings)\n        self.lstm = nn.LSTM(self.embedding.embedding_dim, hidden_dim, batch_first=True, bidirectional=True) # bidirectional=True\n        self.fc = nn.Linear(hidden_dim * 4, output_dim) # Output dimension for binary classification\n\n\n        self.attention = attention_class(\n            decoder_hidden_dim=2*hidden_dim, # Decoder's hidden state is the query for attention\n            encoder_hidden_dim=2*hidden_dim, # Encoder's outputs are the keys/values for attention\n            **attention_kwargs # This will pass specific kwargs like 'attention_dim' if present\n        )\n        self.return_attention_weights = return_attention_weights\n\n\n    def forward(self, input_ids, attention_mask):\n        # 1. Embed the input_ids\n        embeddings = self.embedding(input_ids) #(B,L,E)\n\n        # 2. Get original (non-padded) sequence lengths from attention_mask\n        lengths = attention_mask.sum(dim=1).cpu()\n\n        # 3. Pack the padded sequence\n        packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n            embeddings, lengths, batch_first=True, enforce_sorted=False\n        )\n\n        # 4. Pass the packed sequence through the LSTM\n        # packed_hidden_states: PackedSequence of (B, L, 2*H)\n        # final_hidden_state: (num_layers * num_directions, B, H) -> (2, B, H) for 1-layer bidirectional\n        packed_hidden_states, (final_hidden_state, _) = self.lstm(packed_embeddings)\n            \n        # 5. Unpack the rnn outputs to get all encoder_hidden_states\n        # outputs: (B, L, 2*H) - this contains the concatenated forward and backward hidden states for each time step\n\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n            packed_hidden_states, batch_first=True, total_length=input_ids.size(1)\n        )\n        # For bidirectional LSTMs, final_hidden_state will have shape (2, batch_size, hidden_dim)\n        #6 Squeeze the final_hidden_state  (2,batch_size,hidden_dim) to (batch_size, 2*hidden_dim) to use as query\n        query = final_hidden_state.view(final_hidden_state.size(1), -1) # shape: (batch_size, hidden_dim * 2)\n        #7Calculate context vector using the plug-and-play attention module\n        context_vector, attention_weights = self.attention(\n            query_state=query, \n            encoder_hidden_states=outputs, \n            attention_mask=attention_mask\n        )\n\n    \n       \n\n\n        # 8. Concatenate the query (final hidden state) with the context vector\n        # query: (B, 2*H)\n        # context_vector: (B, 2*H) - typically, attention outputs context vector of encoder_hidden_dim\n        # combined_representation: (B, 4*H)\n        # This combines the \"sequential summary\" with the \"attention-weighted summary\"\n\n\n        combined_representation = torch.cat((query, context_vector), dim=1)    #(B,2*H)  +(B,2*hidden_dim) = (B,4*hidden_dim)\n        \n\n      \n        # 9. Pass through the final linear layer\n        output = self.fc(combined_representation)  \n\n # Return attention weights for visualization/debugging\n  \n        if self.return_attention_weights:\n        \n            return output, attention_weights\n        else:\n            return output  # Only return the output if weights are not explicitly requested\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.123287Z","iopub.execute_input":"2025-07-08T18:46:30.123495Z","iopub.status.idle":"2025-07-08T18:46:30.145226Z","shell.execute_reply.started":"2025-07-08T18:46:30.123481Z","shell.execute_reply":"2025-07-08T18:46:30.144547Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"\ndevice= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.145881Z","iopub.execute_input":"2025-07-08T18:46:30.146072Z","iopub.status.idle":"2025-07-08T18:46:30.164512Z","shell.execute_reply.started":"2025-07-08T18:46:30.146057Z","shell.execute_reply":"2025-07-08T18:46:30.163973Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"# Sneak through into Model Architectures\n\nWe will look into model architectures to judge the complexity of model by looking through the number of parameters","metadata":{}},{"cell_type":"code","source":"from torchinfo import summary\n\nVanillaModels = [VanillaRNN,VanillaLSTM,VanillaBidirectionalRNN,VanillaBidirectionalLSTM]\n\n\nfor model_class in VanillaModels:\n    model = model_class(embedding_matrix = embedding_matrix_tensor).to(device)\n\n\n    dummy_input_ids = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MAX_LEN), dtype=torch.long).to(device)\n    dummy_attention_mask = torch.ones((batch_size, MAX_LEN), dtype=torch.long).to(device)\n    for i in range(BATCH_SIZE):\n        rand_len = torch.randint(low=1, high=MAX_LEN + 1, size=(1,)).item()\n        dummy_attention_mask[i, rand_len:] = 0\n\n    print(f\"\\n--- Model  Summary  {model_class.__name__} ---\")\n    print(summary(model ,input_data=(dummy_input_ids, dummy_attention_mask), device=str(device)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.165114Z","iopub.execute_input":"2025-07-08T18:46:30.165301Z","iopub.status.idle":"2025-07-08T18:46:30.268852Z","shell.execute_reply.started":"2025-07-08T18:46:30.165281Z","shell.execute_reply":"2025-07-08T18:46:30.268239Z"}},"outputs":[{"name":"stdout","text":"\n--- Model  Summary  VanillaRNN ---\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaRNN                               [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                               [757, 128]                29,440\n├─Linear: 1-3                            [32, 1]                   129\n==========================================================================================\nTotal params: 12,876,069\nTrainable params: 29,569\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 3.26\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.06\nParams size (MB): 51.50\nEstimated Total Size (MB): 53.59\n==========================================================================================\n\n--- Model  Summary  VanillaLSTM ---\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaLSTM                              [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                              [762, 128]                117,760\n├─Linear: 1-3                            [32, 1]                   129\n==========================================================================================\nTotal params: 12,964,389\nTrainable params: 117,889\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 11.90\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.06\nParams size (MB): 51.86\nEstimated Total Size (MB): 53.94\n==========================================================================================\n\n--- Model  Summary  VanillaBidirectionalRNN ---\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaBidirectionalRNN                  [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                               [844, 256]                58,880\n├─Linear: 1-3                            [32, 1]                   257\n==========================================================================================\nTotal params: 12,905,637\nTrainable params: 59,137\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 13.13\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 3.01\nParams size (MB): 51.62\nEstimated Total Size (MB): 54.66\n==========================================================================================\n\n--- Model  Summary  VanillaBidirectionalLSTM ---\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaBidirectionalLSTM                 [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                              [843, 256]                235,520\n├─Linear: 1-3                            [32, 1]                   257\n==========================================================================================\nTotal params: 13,082,277\nTrainable params: 235,777\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 51.24\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 3.01\nParams size (MB): 52.33\nEstimated Total Size (MB): 55.36\n==========================================================================================\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"\nHIDDEN_DIM =128\nattention_types = [BahdanauAttention,LuongDotProductAttention,LuongGeneralAttention,LuongConcatAttention]\nattention_kwargs= [{\"attention_hidden_dim\":HIDDEN_DIM},{},{},{\"attention_hidden_dim\":HIDDEN_DIM}]\nplug_and_play_attention_models = [VanillaRNNWithPlugAndPlayAttention,VanillaBidirectionalRNNWithPlugAndPlayAttention,VanillaLSTMWithPlugAndPlayAttention,VanillaBidirectionalLSTMWithPlugAndPlayAttention]\n\nfor pap_a_model in plug_and_play_attention_models:\n    for attention_type,attention_kwarg in zip(attention_types,attention_kwargs):\n        \n        model = pap_a_model( embedding_matrix_tensor,attention_type,attention_kwarg).to(device)\n\n\n        dummy_input_ids = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MAX_LEN), dtype=torch.long).to(device)\n        dummy_attention_mask = torch.ones((batch_size, MAX_LEN), dtype=torch.long).to(device)\n        for i in range(BATCH_SIZE):\n            rand_len = torch.randint(low=1, high=MAX_LEN + 1, size=(1,)).item()\n            dummy_attention_mask[i, rand_len:] = 0\n\n        print(f\"\\n--- Model  Summary  {pap_a_model.__name__} ---with {attention_type.__name__}\")\n        print(summary(model ,input_data=(dummy_input_ids, dummy_attention_mask), device=str(device)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.270592Z","iopub.execute_input":"2025-07-08T18:46:30.270813Z","iopub.status.idle":"2025-07-08T18:46:30.659397Z","shell.execute_reply.started":"2025-07-08T18:46:30.270796Z","shell.execute_reply":"2025-07-08T18:46:30.658772Z"}},"outputs":[{"name":"stdout","text":"\n--- Model  Summary  VanillaRNNWithPlugAndPlayAttention ---with BahdanauAttention\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaRNNWithPlugAndPlayAttention       [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                               [747, 128]                29,440\n├─BahdanauAttention: 1-3                 [32, 128]                 --\n│    └─Linear: 2-1                       [32, 128]                 16,384\n│    └─Linear: 2-2                       [32, 50, 128]             16,384\n│    └─Linear: 2-3                       [32, 50, 1]               128\n├─Linear: 1-4                            [32, 1]                   257\n==========================================================================================\nTotal params: 12,909,093\nTrainable params: 62,593\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 3.23\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 3.73\nParams size (MB): 51.64\nEstimated Total Size (MB): 55.39\n==========================================================================================\n\n--- Model  Summary  VanillaRNNWithPlugAndPlayAttention ---with LuongDotProductAttention\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaRNNWithPlugAndPlayAttention       [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                               [729, 128]                29,440\n├─LuongDotProductAttention: 1-3          [32, 128]                 --\n├─Linear: 1-4                            [32, 1]                   257\n==========================================================================================\nTotal params: 12,876,197\nTrainable params: 29,697\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 3.16\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.03\nParams size (MB): 51.50\nEstimated Total Size (MB): 53.56\n==========================================================================================\n\n--- Model  Summary  VanillaRNNWithPlugAndPlayAttention ---with LuongGeneralAttention\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaRNNWithPlugAndPlayAttention       [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                               [760, 128]                29,440\n├─LuongGeneralAttention: 1-3             [32, 128]                 --\n│    └─Linear: 2-1                       [32, 128]                 16,384\n├─Linear: 1-4                            [32, 1]                   257\n==========================================================================================\nTotal params: 12,892,581\nTrainable params: 46,081\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 3.28\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.09\nParams size (MB): 51.57\nEstimated Total Size (MB): 53.69\n==========================================================================================\n\n--- Model  Summary  VanillaRNNWithPlugAndPlayAttention ---with LuongConcatAttention\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaRNNWithPlugAndPlayAttention       [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                               [856, 128]                29,440\n├─LuongConcatAttention: 1-3              [32, 128]                 --\n│    └─Linear: 2-1                       [32, 50, 128]             32,768\n│    └─Linear: 2-2                       [32, 50, 1]               128\n├─Linear: 1-4                            [32, 1]                   257\n==========================================================================================\nTotal params: 12,909,093\nTrainable params: 62,593\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 3.64\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 3.81\nParams size (MB): 51.64\nEstimated Total Size (MB): 55.47\n==========================================================================================\n\n--- Model  Summary  VanillaBidirectionalRNNWithPlugAndPlayAttention ---with BahdanauAttention\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nVanillaBidirectionalRNNWithPlugAndPlayAttention    [32, 1]                   --\n├─Embedding: 1-1                                   [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                                         [752, 256]                58,880\n├─BahdanauAttention: 1-3                           [32, 256]                 --\n│    └─Linear: 2-1                                 [32, 128]                 32,768\n│    └─Linear: 2-2                                 [32, 50, 128]             32,768\n│    └─Linear: 2-3                                 [32, 50, 1]               128\n├─Linear: 1-4                                      [32, 1]                   513\n====================================================================================================\nTotal params: 12,971,557\nTrainable params: 125,057\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 11.75\n====================================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 4.50\nParams size (MB): 51.89\nEstimated Total Size (MB): 56.42\n====================================================================================================\n\n--- Model  Summary  VanillaBidirectionalRNNWithPlugAndPlayAttention ---with LuongDotProductAttention\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nVanillaBidirectionalRNNWithPlugAndPlayAttention    [32, 1]                   --\n├─Embedding: 1-1                                   [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                                         [685, 256]                58,880\n├─LuongDotProductAttention: 1-3                    [32, 256]                 --\n├─Linear: 1-4                                      [32, 1]                   513\n====================================================================================================\nTotal params: 12,905,893\nTrainable params: 59,393\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 10.74\n====================================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.68\nParams size (MB): 51.62\nEstimated Total Size (MB): 54.33\n====================================================================================================\n\n--- Model  Summary  VanillaBidirectionalRNNWithPlugAndPlayAttention ---with LuongGeneralAttention\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nVanillaBidirectionalRNNWithPlugAndPlayAttention    [32, 1]                   --\n├─Embedding: 1-1                                   [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                                         [776, 256]                58,880\n├─LuongGeneralAttention: 1-3                       [32, 256]                 --\n│    └─Linear: 2-1                                 [32, 256]                 65,536\n├─Linear: 1-4                                      [32, 1]                   513\n====================================================================================================\nTotal params: 12,971,429\nTrainable params: 124,929\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 12.11\n====================================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.94\nParams size (MB): 51.89\nEstimated Total Size (MB): 54.85\n====================================================================================================\n\n--- Model  Summary  VanillaBidirectionalRNNWithPlugAndPlayAttention ---with LuongConcatAttention\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nVanillaBidirectionalRNNWithPlugAndPlayAttention    [32, 1]                   --\n├─Embedding: 1-1                                   [32, 50, 100]             (12,846,500)\n├─RNN: 1-2                                         [618, 256]                58,880\n├─LuongConcatAttention: 1-3                        [32, 256]                 --\n│    └─Linear: 2-1                                 [32, 50, 128]             65,536\n│    └─Linear: 2-2                                 [32, 50, 1]               128\n├─Linear: 1-4                                      [32, 1]                   513\n====================================================================================================\nTotal params: 12,971,557\nTrainable params: 125,057\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 9.73\n====================================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 4.20\nParams size (MB): 51.89\nEstimated Total Size (MB): 56.11\n====================================================================================================\n\n--- Model  Summary  VanillaLSTMWithPlugAndPlayAttention ---with BahdanauAttention\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaLSTMWithPlugAndPlayAttention      [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                              [898, 128]                117,760\n├─BahdanauAttention: 1-3                 [32, 128]                 --\n│    └─Linear: 2-1                       [32, 128]                 16,384\n│    └─Linear: 2-2                       [32, 50, 128]             16,384\n│    └─Linear: 2-3                       [32, 50, 1]               128\n├─Linear: 1-4                            [32, 1]                   257\n==========================================================================================\nTotal params: 12,997,413\nTrainable params: 150,913\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 13.95\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 3.88\nParams size (MB): 51.99\nEstimated Total Size (MB): 55.90\n==========================================================================================\n\n--- Model  Summary  VanillaLSTMWithPlugAndPlayAttention ---with LuongDotProductAttention\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaLSTMWithPlugAndPlayAttention      [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                              [794, 128]                117,760\n├─LuongDotProductAttention: 1-3          [32, 128]                 --\n├─Linear: 1-4                            [32, 1]                   257\n==========================================================================================\nTotal params: 12,964,517\nTrainable params: 118,017\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 12.38\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.09\nParams size (MB): 51.86\nEstimated Total Size (MB): 53.98\n==========================================================================================\n\n--- Model  Summary  VanillaLSTMWithPlugAndPlayAttention ---with LuongGeneralAttention\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaLSTMWithPlugAndPlayAttention      [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                              [758, 128]                117,760\n├─LuongGeneralAttention: 1-3             [32, 128]                 --\n│    └─Linear: 2-1                       [32, 128]                 16,384\n├─Linear: 1-4                            [32, 1]                   257\n==========================================================================================\nTotal params: 12,980,901\nTrainable params: 134,401\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 11.84\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 2.09\nParams size (MB): 51.92\nEstimated Total Size (MB): 54.04\n==========================================================================================\n\n--- Model  Summary  VanillaLSTMWithPlugAndPlayAttention ---with LuongConcatAttention\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nVanillaLSTMWithPlugAndPlayAttention      [32, 1]                   --\n├─Embedding: 1-1                         [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                              [857, 128]                117,760\n├─LuongConcatAttention: 1-3              [32, 128]                 --\n│    └─Linear: 2-1                       [32, 50, 128]             32,768\n│    └─Linear: 2-2                       [32, 50, 1]               128\n├─Linear: 1-4                            [32, 1]                   257\n==========================================================================================\nTotal params: 12,997,413\nTrainable params: 150,913\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 13.33\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 3.81\nParams size (MB): 51.99\nEstimated Total Size (MB): 55.82\n==========================================================================================\n\n--- Model  Summary  VanillaBidirectionalLSTMWithPlugAndPlayAttention ---with BahdanauAttention\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nVanillaBidirectionalLSTMWithPlugAndPlayAttention   [32, 1]                   --\n├─Embedding: 1-1                                   [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                                        [941, 256]                235,520\n├─BahdanauAttention: 1-3                           [32, 256]                 --\n│    └─Linear: 2-1                                 [32, 128]                 32,768\n│    └─Linear: 2-2                                 [32, 50, 128]             32,768\n│    └─Linear: 2-3                                 [32, 50, 1]               128\n├─Linear: 1-4                                      [32, 1]                   513\n====================================================================================================\nTotal params: 13,148,197\nTrainable params: 301,697\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 57.15\n====================================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 4.89\nParams size (MB): 52.59\nEstimated Total Size (MB): 57.51\n====================================================================================================\n\n--- Model  Summary  VanillaBidirectionalLSTMWithPlugAndPlayAttention ---with LuongDotProductAttention\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nVanillaBidirectionalLSTMWithPlugAndPlayAttention   [32, 1]                   --\n├─Embedding: 1-1                                   [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                                        [991, 256]                235,520\n├─LuongDotProductAttention: 1-3                    [32, 256]                 --\n├─Linear: 1-4                                      [32, 1]                   513\n====================================================================================================\nTotal params: 13,082,533\nTrainable params: 236,033\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 60.16\n====================================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 3.31\nParams size (MB): 52.33\nEstimated Total Size (MB): 55.67\n====================================================================================================\n\n--- Model  Summary  VanillaBidirectionalLSTMWithPlugAndPlayAttention ---with LuongGeneralAttention\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nVanillaBidirectionalLSTMWithPlugAndPlayAttention   [32, 1]                   --\n├─Embedding: 1-1                                   [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                                        [977, 256]                235,520\n├─LuongGeneralAttention: 1-3                       [32, 256]                 --\n│    └─Linear: 2-1                                 [32, 256]                 65,536\n├─Linear: 1-4                                      [32, 1]                   513\n====================================================================================================\nTotal params: 13,148,069\nTrainable params: 301,569\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 59.32\n====================================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 3.35\nParams size (MB): 52.59\nEstimated Total Size (MB): 55.96\n====================================================================================================\n\n--- Model  Summary  VanillaBidirectionalLSTMWithPlugAndPlayAttention ---with LuongConcatAttention\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nVanillaBidirectionalLSTMWithPlugAndPlayAttention   [32, 1]                   --\n├─Embedding: 1-1                                   [32, 50, 100]             (12,846,500)\n├─LSTM: 1-2                                        [809, 256]                235,520\n├─LuongConcatAttention: 1-3                        [32, 256]                 --\n│    └─Linear: 2-1                                 [32, 50, 128]             65,536\n│    └─Linear: 2-2                                 [32, 50, 1]               128\n├─Linear: 1-4                                      [32, 1]                   513\n====================================================================================================\nTotal params: 13,148,197\nTrainable params: 301,697\nNon-trainable params: 12,846,500\nTotal mult-adds (Units.GIGABYTES): 49.19\n====================================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 4.59\nParams size (MB): 52.59\nEstimated Total Size (MB): 57.21\n====================================================================================================\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"for attention,attention_kwarg in zip(attention_types,attention_kwargs):\n    print(attention, \"   \", attention_kwarg)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.660077Z","iopub.execute_input":"2025-07-08T18:46:30.660259Z","iopub.status.idle":"2025-07-08T18:46:30.664406Z","shell.execute_reply.started":"2025-07-08T18:46:30.660245Z","shell.execute_reply":"2025-07-08T18:46:30.663738Z"}},"outputs":[{"name":"stdout","text":"<class '__main__.BahdanauAttention'>     {'attention_hidden_dim': 128}\n<class '__main__.LuongDotProductAttention'>     {}\n<class '__main__.LuongGeneralAttention'>     {}\n<class '__main__.LuongConcatAttention'>     {'attention_hidden_dim': 128}\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"# After this the traning procedure begins !","metadata":{}},{"cell_type":"markdown","source":"## important parameters ","metadata":{}},{"cell_type":"code","source":"\n\n\nlearning_rate =  0.01\nepochs = 30\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.665129Z","iopub.execute_input":"2025-07-08T18:46:30.665393Z","iopub.status.idle":"2025-07-08T18:46:30.682928Z","shell.execute_reply.started":"2025-07-08T18:46:30.665356Z","shell.execute_reply":"2025-07-08T18:46:30.682238Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# Training Pipeline","metadata":{"execution":{"iopub.execute_input":"2025-07-08T11:12:19.355040Z","iopub.status.busy":"2025-07-08T11:12:19.354712Z","iopub.status.idle":"2025-07-08T11:12:19.360203Z","shell.execute_reply":"2025-07-08T11:12:19.359112Z","shell.execute_reply.started":"2025-07-08T11:12:19.355018Z"}}},{"cell_type":"code","source":"\ndef train_model(model, learning_rate, epochs, train_dataloader, val_dataloader, model_save_dir=None, patience=5):\n    \"\"\"\n    Trains a PyTorch model with early stopping, saves the best model,\n    and logs/plots training and validation loss.\n\n    Args:\n        model (torch.nn.Module): The PyTorch model to train.\n        learning_rate (float): The learning rate for the optimizer.\n        epochs (int): The maximum number of training epochs.\n        train_dataloader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_dataloader (torch.utils.data.DataLoader): DataLoader for validation data.\n        model_save_dir (str, optional): The directory to save the model, logs, and plots.\n                                        Example: \"/kaggle/working/my_model_folder\"\n                                        If None, nothing will be saved.\n        patience (int): Number of epochs to wait for improvement before stopping.\n    \"\"\"\n    model.train()\n    model = model.to(device)\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Variables for early stopping\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    best_model_state = None\n    best_epoch = -1 # To track which epoch had the best validation loss\n\n    # Lists to store metrics for plotting and logging\n    train_losses = []\n    val_losses = []\n\n    # Create the model directory if it's provided\n    if model_save_dir:\n        os.makedirs(model_save_dir, exist_ok=True)\n        print(f\"Model saving directory created/ensured: {model_save_dir}\")\n    start_time = time.time() # Record start time\n    for epoch in range(epochs):\n        # --- Training Phase ---\n        model.train() # Set model to training mode\n        current_train_loss = 0.0\n        for batch_idx, (batch_input_ids, batch_attention_mask, batch_labels) in enumerate(train_dataloader):\n            # Move data to the correct device\n            batch_input_ids = batch_input_ids.to(device)\n            batch_labels = batch_labels.to(device)\n            batch_attention_mask = batch_attention_mask.to(device)\n\n            # Forward pass\n            output = model.forward(batch_input_ids, batch_attention_mask)\n            loss = criterion(output, batch_labels.view(-1, 1).float())\n\n            # Backward pass and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            current_train_loss += loss.item()\n\n            # Optional: print per-batch loss if needed, but it can be very verbose\n            # print(f\"Epoch {epoch+1} | Batch {batch_idx+1}/{len(train_dataloader)} | Train Batch Loss: {loss.item():.4f}\")\n\n        avg_train_loss = current_train_loss / len(train_dataloader)\n        train_losses.append(avg_train_loss)\n        print(f'Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f}')\n\n        # --- Validation Phase ---\n        model.eval() # Set model to evaluation mode\n        current_val_loss = 0.0\n        with torch.no_grad(): # Disable gradient calculations for validation\n            for batch_idx, (batch_input_ids, batch_attention_mask, batch_labels) in enumerate(val_dataloader):\n                batch_input_ids = batch_input_ids.to(device)\n                batch_labels = batch_labels.to(device)\n                batch_attention_mask = batch_attention_mask.to(device)\n\n                output = model.forward(batch_input_ids, batch_attention_mask)\n                loss = criterion(output, batch_labels.view(-1, 1).float())\n                current_val_loss += loss.item()\n\n        avg_val_loss = current_val_loss / len(val_dataloader)\n        val_losses.append(avg_val_loss)\n        print(f'Epoch {epoch+1}/{epochs} | Val Loss: {avg_val_loss:.4f}')\n\n        # --- Early Stopping Logic ---\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            epochs_no_improve = 0\n            best_epoch = epoch + 1\n            # Save the best model state dictionary\n            best_model_state = model.state_dict()\n            print(f\"Validation loss improved. Current best: {best_val_loss:.4f} (Epoch {best_epoch}).\")\n        else:\n            epochs_no_improve += 1\n            print(f\"Validation loss did not improve for {epochs_no_improve} epoch(s). Best was {best_val_loss:.4f} at Epoch {best_epoch}.\")\n            if epochs_no_improve >= patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs (patience: {patience}).\")\n                break # Exit the training loop\n\n    # --- Post-training actions: Saving and Plotting ---\n    end_time = time.time() # Record end time\n    training_time_seconds = end_time - start_time\n    print(f\"Finished training for {model.__class__.__name__}. Total time: {training_time_seconds:.2f} seconds.\")\n\n    \n    # Save the best model after training (either all epochs or early stopped)\n    if model_save_dir and best_model_state is not None:\n        model_path = os.path.join(model_save_dir, \"best_model.pth\")\n        torch.save(best_model_state, model_path)\n        print(f\"Best model (based on validation loss) saved successfully to: {model_path}\")\n    elif model_save_dir is None:\n        print(\"Model not saved (no model_save_dir provided).\")\n    else:\n        print(\"Model not saved as validation loss never improved or no best state was recorded.\")\n\n    # Save training logs\n    if model_save_dir:\n        log_data = pd.DataFrame({\n            'Epoch': range(1, len(train_losses) + 1),\n            'Train_Loss': train_losses,\n            'Val_Loss': val_losses\n        })\n        log_path = os.path.join(model_save_dir, \"training_log.csv\")\n        log_data.to_csv(log_path, index=False)\n        print(f\"Training logs saved to: {log_path}\")\n\n        # Plotting the training and validation loss\n        plt.figure(figsize=(10, 6))\n        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', marker='o', linestyle='-')\n        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', marker='x', linestyle='--')\n\n        # Add a vertical line for the best epoch if early stopping occurred\n        if best_epoch != -1 and best_epoch <= len(train_losses): # Ensure best_epoch is within plotted range\n             plt.axvline(x=best_epoch, color='r', linestyle=':', label=f'Best Val Loss (Epoch {best_epoch})')\n\n\n        plt.title('Training and Validation Loss Over Epochs', fontsize=16)\n        plt.xlabel('Epoch', fontsize=12)\n        plt.ylabel('Loss', fontsize=12)\n        plt.legend(fontsize=10)\n        plt.grid(True, linestyle='--', alpha=0.7)\n        plt.xticks(range(1, len(train_losses) + 1)) # Ensure integer ticks for epochs\n        plt.tight_layout() # Adjust layout to prevent labels from overlapping\n        plot_path = os.path.join(model_save_dir, \"loss_plot.png\")\n        plt.savefig(plot_path)\n        print(f\"Loss plot saved to: {plot_path}\")\n        plt.close() # Close the plot to free memory\n\n    # Load the best model state if it was saved, before returning\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n        model.to(device) # Ensure the loaded model is on the correct device\n        print(\"Loaded best model state for return.\")\n    else:\n        print(\"No best model state recorded or loaded.\")\n\n    return model, training_time_seconds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.683563Z","iopub.execute_input":"2025-07-08T18:46:30.683866Z","iopub.status.idle":"2025-07-08T18:46:30.700778Z","shell.execute_reply.started":"2025-07-08T18:46:30.683846Z","shell.execute_reply":"2025-07-08T18:46:30.700208Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"# Getting and Intepreting Results Pipeline","metadata":{}},{"cell_type":"code","source":"def get_statistics(model, test_dataloader, threshold=0.5):\n    \"\"\"\n    Evaluates the model on the test_dataloader and returns various classification metrics.\n\n    Args:\n        model (torch.nn.Module): The trained PyTorch model.\n        test_dataloader (torch.utils.data.DataLoader): DataLoader for the test set.\n        threshold (float): The threshold for converting sigmoid output to binary predictions.\n                           Defaults to 0.5.\n\n    Returns:\n        dict: A dictionary containing:\n              - 'accuracy': Overall accuracy.\n              - 'precision': Precision score.\n              - 'recall': Recall score.\n              - 'f1_score': F1-score.\n              - 'confusion_matrix': A 2x2 NumPy array representing the confusion matrix.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n\n    # Initialize TorchMetrics for binary classification\n    # Ensure task=\"binary\" and threshold are set, and move metrics to the device\n    metric_accuracy = Accuracy(task=\"binary\", threshold=threshold).to(device)\n    metric_precision = Precision(task=\"binary\", threshold=threshold).to(device)\n    metric_recall = Recall(task=\"binary\", threshold=threshold).to(device)\n    metric_f1 = F1Score(task=\"binary\", threshold=threshold).to(device)\n    metric_confusion_matrix = BinaryConfusionMatrix(threshold=threshold).to(device)\n\n    with torch.no_grad():  # Disable gradient calculation for evaluation\n        for batch_input_ids, batch_attention_mask, batch_labels in test_dataloader:\n            # Move data to the correct device\n            batch_input_ids = batch_input_ids.to(device)\n            batch_attention_mask = batch_attention_mask.to(device)\n            batch_labels = batch_labels.to(device)\n\n            # Forward pass: model should accept both input_ids and attention_mask\n            output_logits = model(batch_input_ids,batch_attention_mask)\n            \n            # Apply sigmoid to get probabilities. Squeeze to remove the last dimension (e.g., from [batch_size, 1] to [batch_size])\n            output_probs = F.sigmoid(output_logits).squeeze(1)\n\n            # Update each metric with the predicted probabilities and ground truth labels\n            # Note: TorchMetrics expects target labels to be of type torch.long\n            metric_accuracy.update(output_probs, batch_labels.long())\n            metric_precision.update(output_probs, batch_labels.long())\n            metric_recall.update(output_probs, batch_labels.long())\n            metric_f1.update(output_probs, batch_labels.long())\n            metric_confusion_matrix.update(output_probs, batch_labels.long())\n\n    # Compute final metrics from the accumulated states\n    accuracy = metric_accuracy.compute().item()\n    precision = metric_precision.compute().item()\n    recall = metric_recall.compute().item()\n    f1_score = metric_f1.compute().item()\n    \n    # Confusion matrix is a tensor, move to CPU and convert to NumPy for printing/return\n    confusion_matrix_tensor = metric_confusion_matrix.compute()\n    confusion_matrix_np = confusion_matrix_tensor.cpu().numpy()\n\n    # Print the results\n    print(\"\\n--- Classification Statistics ---\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-score: {f1_score:.4f}\")\n\n    print(\"\\nConfusion Matrix:\")\n    # For binary classification, a 2x2 matrix is typically structured as:\n    # [[True Negatives, False Positives],\n    #  [False Negatives, True Positives]]\n    cm_df = pd.DataFrame(confusion_matrix_np,\n                         index=['Actual Negative (0)', 'Actual Positive (1)'],\n                         columns=['Predicted Negative (0)', 'Predicted Positive (1)'])\n    print(cm_df)\n\n    # Return the results as a dictionary\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'confusion_matrix': confusion_matrix_np\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.701554Z","iopub.execute_input":"2025-07-08T18:46:30.702276Z","iopub.status.idle":"2025-07-08T18:46:30.718932Z","shell.execute_reply.started":"2025-07-08T18:46:30.702254Z","shell.execute_reply":"2025-07-08T18:46:30.718285Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"# Final Traning of All Models!","metadata":{}},{"cell_type":"code","source":"\n# Define where to save models and log results\nmodel_save_base_dir = \"/kaggle/working/saved_models\"\nos.makedirs(model_save_base_dir, exist_ok=True) # Ensure base directory exists\n\nresults_log_file_path = \"/kaggle/working/model_results.csv\"\n\n\n\n# --- Global header check for CSV logging ---\nwrite_header = not os.path.exists(results_log_file_path)\nif write_header:\n    with open(results_log_file_path, 'w') as f_log: # 'w' to create/overwrite if it doesn't exist\n        header = \"Model_Name,Accuracy,Precision,Recall,F1_Score,CM_TN,CM_FP,CM_FN,CM_TP,Training_Time_Sec,Trainable_Params\\n\"\n        f_log.write(header)\n    print(f\"Created new results log file: {results_log_file_path} with header.\")\nelse:\n    print(f\"Appending to existing results log file: {results_log_file_path}.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.719519Z","iopub.execute_input":"2025-07-08T18:46:30.719697Z","iopub.status.idle":"2025-07-08T18:46:30.736698Z","shell.execute_reply.started":"2025-07-08T18:46:30.719684Z","shell.execute_reply":"2025-07-08T18:46:30.736134Z"}},"outputs":[{"name":"stdout","text":"Created new results log file: /kaggle/working/model_results.csv with header.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"def count_parameters(model):\n    \"\"\"Counts the number of trainable parameters in a PyTorch model.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:46:30.737346Z","iopub.execute_input":"2025-07-08T18:46:30.737562Z","iopub.status.idle":"2025-07-08T18:46:30.752238Z","shell.execute_reply.started":"2025-07-08T18:46:30.737548Z","shell.execute_reply":"2025-07-08T18:46:30.751688Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"\n\nVanillaModels = [VanillaLSTM,VanillaBidirectionalRNN,VanillaBidirectionalLSTM]\n# VanillaModels = [VanillaRNN]\n\n\nfor model_class in VanillaModels:\n    print(f\"\\n--- Training {model_class.__name__} ---\")\n    model = model_class(embedding_matrix = embedding_matrix_tensor).to(device)\n    model_name = model_class.__name__\n    model_save_dir = os.path.join(model_save_base_dir, model_name)\n\n    best_model , training_time_seconds = train_model(model,learning_rate,epochs,train_dataloader,val_dataloader,model_save_dir=model_save_dir)\n    #\n    # Call the function and get the results dictionary\n    results= get_statistics(best_model, test_dataloader, 0.5)\n   \n    # Calculate trainable parameters\n    num_trainable_params = count_parameters(best_model)\n# Extract individual metrics from the results dictionary\n    accuracy = results['accuracy']\n    precision = results['precision']\n    recall = results['recall']\n    f1_score = results['f1_score']\n    confusion_matrix_array = results['confusion_matrix'] # Get the NumPy array\n    \n    \n    # Flatten the confusion matrix for logging (TN, FP, FN, TP)\n    # Assuming binary classification with cm = [[TN, FP], [FN, TP]]\n    cm_tn, cm_fp, cm_fn, cm_tp = confusion_matrix_array.ravel()\n\n     # Prepare the data row for the log file\n    log_line = (f\"{model_name},\"\n                    f\"{accuracy:.4f},\"\n                    f\"{precision:.4f},\"\n                    f\"{recall:.4f},\"\n                    f\"{f1_score:.4f},\"\n                    f\"{cm_tn},\"\n                    f\"{cm_fp},\"\n                    f\"{cm_fn},\"\n                    f\"{cm_tp},\"\n                f\"{training_time_seconds:.2f},\"  # New: Training Time\n                f\"{num_trainable_params}\\n\")      # New: Trainable Parameters\n\n \n\n    #Append to log file\n    with open(results_log_file_path, 'a') as f_log:\n        f_log.write(log_line)\n\n    print(f\"Logged results for {model_name} to {results_log_file_path}\")\n    print(f\"--- Finished training and logging for model: {model_class.__name__} ---\")\n\n\nprint(\"\\nAll Vanilla models trained and results logged.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T19:11:51.553392Z","iopub.execute_input":"2025-07-08T19:11:51.554125Z","iopub.status.idle":"2025-07-08T19:28:33.470813Z","shell.execute_reply.started":"2025-07-08T19:11:51.554102Z","shell.execute_reply":"2025-07-08T19:28:33.469867Z"}},"outputs":[{"name":"stdout","text":"\n--- Training VanillaLSTM ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaLSTM\nEpoch 1/30 | Train Loss: 0.4023\nEpoch 1/30 | Val Loss: 0.3472\nValidation loss improved. Current best: 0.3472 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.3254\nEpoch 2/30 | Val Loss: 0.3271\nValidation loss improved. Current best: 0.3271 (Epoch 2).\nEpoch 3/30 | Train Loss: 0.2947\nEpoch 3/30 | Val Loss: 0.3238\nValidation loss improved. Current best: 0.3238 (Epoch 3).\nEpoch 4/30 | Train Loss: 0.2723\nEpoch 4/30 | Val Loss: 0.3348\nValidation loss did not improve for 1 epoch(s). Best was 0.3238 at Epoch 3.\nEpoch 5/30 | Train Loss: 0.2542\nEpoch 5/30 | Val Loss: 0.3285\nValidation loss did not improve for 2 epoch(s). Best was 0.3238 at Epoch 3.\nEpoch 6/30 | Train Loss: 0.2359\nEpoch 6/30 | Val Loss: 0.3314\nValidation loss did not improve for 3 epoch(s). Best was 0.3238 at Epoch 3.\nEpoch 7/30 | Train Loss: 0.2262\nEpoch 7/30 | Val Loss: 0.3463\nValidation loss did not improve for 4 epoch(s). Best was 0.3238 at Epoch 3.\nEpoch 8/30 | Train Loss: 0.2161\nEpoch 8/30 | Val Loss: 0.3471\nValidation loss did not improve for 5 epoch(s). Best was 0.3238 at Epoch 3.\nEarly stopping triggered after 8 epochs (patience: 5).\nFinished training for VanillaLSTM. Total time: 157.44 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaLSTM/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaLSTM/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaLSTM/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.8526\nPrecision: 0.8341\nRecall: 0.8840\nF1-score: 0.8583\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                    2031                     444\nActual Positive (1)                     293                    2232\nLogged results for VanillaLSTM to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaLSTM ---\n\n--- Training VanillaBidirectionalRNN ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaBidirectionalRNN\nEpoch 1/30 | Train Loss: 0.7383\nEpoch 1/30 | Val Loss: 0.7364\nValidation loss improved. Current best: 0.7364 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.7391\nEpoch 2/30 | Val Loss: 0.7295\nValidation loss improved. Current best: 0.7295 (Epoch 2).\nEpoch 3/30 | Train Loss: 0.7447\nEpoch 3/30 | Val Loss: 0.7087\nValidation loss improved. Current best: 0.7087 (Epoch 3).\nEpoch 4/30 | Train Loss: 0.7404\nEpoch 4/30 | Val Loss: 0.7043\nValidation loss improved. Current best: 0.7043 (Epoch 4).\nEpoch 5/30 | Train Loss: 0.7395\nEpoch 5/30 | Val Loss: 0.9038\nValidation loss did not improve for 1 epoch(s). Best was 0.7043 at Epoch 4.\nEpoch 6/30 | Train Loss: 0.7377\nEpoch 6/30 | Val Loss: 0.7092\nValidation loss did not improve for 2 epoch(s). Best was 0.7043 at Epoch 4.\nEpoch 7/30 | Train Loss: 0.7404\nEpoch 7/30 | Val Loss: 0.7434\nValidation loss did not improve for 3 epoch(s). Best was 0.7043 at Epoch 4.\nEpoch 8/30 | Train Loss: 0.7393\nEpoch 8/30 | Val Loss: 0.7612\nValidation loss did not improve for 4 epoch(s). Best was 0.7043 at Epoch 4.\nEpoch 9/30 | Train Loss: 0.7305\nEpoch 9/30 | Val Loss: 0.7391\nValidation loss did not improve for 5 epoch(s). Best was 0.7043 at Epoch 4.\nEarly stopping triggered after 9 epochs (patience: 5).\nFinished training for VanillaBidirectionalRNN. Total time: 254.00 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaBidirectionalRNN/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaBidirectionalRNN/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaBidirectionalRNN/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.4894\nPrecision: 0.4944\nRecall: 0.4859\nF1-score: 0.4901\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                    1220                    1255\nActual Positive (1)                    1298                    1227\nLogged results for VanillaBidirectionalRNN to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaBidirectionalRNN ---\n\n--- Training VanillaBidirectionalLSTM ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaBidirectionalLSTM\nEpoch 1/30 | Train Loss: 0.6972\nEpoch 1/30 | Val Loss: 0.7011\nValidation loss improved. Current best: 0.7011 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.6975\nEpoch 2/30 | Val Loss: 0.6969\nValidation loss improved. Current best: 0.6969 (Epoch 2).\nEpoch 3/30 | Train Loss: 0.6964\nEpoch 3/30 | Val Loss: 0.6968\nValidation loss improved. Current best: 0.6968 (Epoch 3).\nEpoch 4/30 | Train Loss: 0.6976\nEpoch 4/30 | Val Loss: 0.6969\nValidation loss did not improve for 1 epoch(s). Best was 0.6968 at Epoch 3.\nEpoch 5/30 | Train Loss: 0.6963\nEpoch 5/30 | Val Loss: 0.6965\nValidation loss improved. Current best: 0.6965 (Epoch 5).\nEpoch 6/30 | Train Loss: 0.6964\nEpoch 6/30 | Val Loss: 0.6997\nValidation loss did not improve for 1 epoch(s). Best was 0.6965 at Epoch 5.\nEpoch 7/30 | Train Loss: 0.6963\nEpoch 7/30 | Val Loss: 0.6956\nValidation loss improved. Current best: 0.6956 (Epoch 7).\nEpoch 8/30 | Train Loss: 0.6963\nEpoch 8/30 | Val Loss: 0.6951\nValidation loss improved. Current best: 0.6951 (Epoch 8).\nEpoch 9/30 | Train Loss: 0.6961\nEpoch 9/30 | Val Loss: 0.6976\nValidation loss did not improve for 1 epoch(s). Best was 0.6951 at Epoch 8.\nEpoch 10/30 | Train Loss: 0.6962\nEpoch 10/30 | Val Loss: 0.6948\nValidation loss improved. Current best: 0.6948 (Epoch 10).\nEpoch 11/30 | Train Loss: 0.6960\nEpoch 11/30 | Val Loss: 0.6976\nValidation loss did not improve for 1 epoch(s). Best was 0.6948 at Epoch 10.\nEpoch 12/30 | Train Loss: 0.6962\nEpoch 12/30 | Val Loss: 0.6946\nValidation loss improved. Current best: 0.6946 (Epoch 12).\nEpoch 13/30 | Train Loss: 0.6960\nEpoch 13/30 | Val Loss: 0.6940\nValidation loss improved. Current best: 0.6940 (Epoch 13).\nEpoch 14/30 | Train Loss: 0.6953\nEpoch 14/30 | Val Loss: 0.6940\nValidation loss improved. Current best: 0.6940 (Epoch 14).\nEpoch 15/30 | Train Loss: 0.6962\nEpoch 15/30 | Val Loss: 0.6956\nValidation loss did not improve for 1 epoch(s). Best was 0.6940 at Epoch 14.\nEpoch 16/30 | Train Loss: 0.6960\nEpoch 16/30 | Val Loss: 0.6945\nValidation loss did not improve for 2 epoch(s). Best was 0.6940 at Epoch 14.\nEpoch 17/30 | Train Loss: 0.6953\nEpoch 17/30 | Val Loss: 0.6976\nValidation loss did not improve for 3 epoch(s). Best was 0.6940 at Epoch 14.\nEpoch 18/30 | Train Loss: 0.6956\nEpoch 18/30 | Val Loss: 0.6962\nValidation loss did not improve for 4 epoch(s). Best was 0.6940 at Epoch 14.\nEpoch 19/30 | Train Loss: 0.6954\nEpoch 19/30 | Val Loss: 0.6949\nValidation loss did not improve for 5 epoch(s). Best was 0.6940 at Epoch 14.\nEarly stopping triggered after 19 epochs (patience: 5).\nFinished training for VanillaBidirectionalLSTM. Total time: 582.90 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaBidirectionalLSTM/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaBidirectionalLSTM/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaBidirectionalLSTM/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.5090\nPrecision: 0.5103\nRecall: 0.6875\nF1-score: 0.5858\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                     809                    1666\nActual Positive (1)                     789                    1736\nLogged results for VanillaBidirectionalLSTM to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaBidirectionalLSTM ---\n\nAll Vanilla models trained and results logged.\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:54:28.891670Z","iopub.execute_input":"2025-07-08T18:54:28.892538Z","iopub.status.idle":"2025-07-08T18:54:28.895844Z","shell.execute_reply.started":"2025-07-08T18:54:28.892509Z","shell.execute_reply":"2025-07-08T18:54:28.895247Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:56:02.591582Z","iopub.execute_input":"2025-07-08T18:56:02.592190Z","iopub.status.idle":"2025-07-08T18:56:02.608913Z","shell.execute_reply.started":"2025-07-08T18:56:02.592167Z","shell.execute_reply":"2025-07-08T18:56:02.608332Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"['movie',\n 'utterly',\n 'boring',\n 'complete',\n 'waste',\n 'time',\n 'regret',\n 'watching']"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T18:57:56.145060Z","iopub.execute_input":"2025-07-08T18:57:56.145555Z","iopub.status.idle":"2025-07-08T18:57:56.150535Z","shell.execute_reply.started":"2025-07-08T18:57:56.145531Z","shell.execute_reply":"2025-07-08T18:57:56.149822Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"[3, 1091, 188, 385, 253, 10, 2193, 41]"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T19:36:01.661174Z","iopub.execute_input":"2025-07-08T19:36:01.661520Z","iopub.status.idle":"2025-07-08T19:36:01.700595Z","shell.execute_reply.started":"2025-07-08T19:36:01.661491Z","shell.execute_reply":"2025-07-08T19:36:01.699411Z"}},"outputs":[{"name":"stdout","text":"0.5038464665412903\nPositive\n","output_type":"stream"}],"execution_count":92},{"cell_type":"markdown","source":"## For models with Attention","metadata":{}},{"cell_type":"code","source":"# # --- Loop for Attention-Based Models ---\nprint(\"\\n--- Starting training and evaluation for Attention-Based Models ---\")\nattention_types = [BahdanauAttention, LuongDotProductAttention, LuongGeneralAttention, LuongConcatAttention]\n# Ensure attention_kwargs match the attention_types and their constructors\nattention_kwargs = [\n    {\"attention_hidden_dim\": HIDDEN_DIM}, # Bahdanau\n    {},                                   # LuongDotProduct\n    {},                                   # LuongGeneral\n    {\"attention_hidden_dim\": HIDDEN_DIM}  # LuongConcat\n]\nplug_and_play_attention_models = [\n    VanillaRNNWithPlugAndPlayAttention,\n    VanillaBidirectionalRNNWithPlugAndPlayAttention,\n]\n\nfor pap_a_model in plug_and_play_attention_models:\n    for attention_type, attention_kwarg in zip(attention_types, attention_kwargs):\n        base_model_name = pap_a_model.__name__.replace(\"WithPlugAndPlayAttention\", \"\")\n        attn_name = attention_type.__name__.replace(\"Attention\", \"\")\n        full_model_name = f\"{base_model_name}With{attn_name}Attention\"\n        \n        print(f\"\\n--- Training {full_model_name} ---\")\n\n        # Instantiate model\n        # return_attention_weights=False is default, good for training\n        model = pap_a_model(\n            embedding_matrix=embedding_matrix_tensor,\n            attention_class=attention_type,\n            attention_kwargs=attention_kwarg,\n            hidden_dim=HIDDEN_DIM,\n            freeze_embeddings=True\n        ).to(device)\n        num_trainable_params = count_parameters(best_model)\n        # # Print model summary for each attention model combination\n        # # Use a consistent dummy input for summary\n        # dummy_input_ids_summary = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MAX_LEN), dtype=torch.long).to(device)\n        # dummy_attention_mask_summary = torch.ones((BATCH_SIZE, MAX_LEN), dtype=torch.long).to(device)\n        # for i in range(BATCH_SIZE):\n        #     rand_len = torch.randint(low=1, high=MAX_LEN + 1, size=(1,)).item()\n        #     dummy_attention_mask_summary[i, rand_len:] = 0\n        \n        # print(f\"\\n--- Model Summary {full_model_name} --- with {attention_type.__name__}\")\n        # # Note: If your attention models return a tuple (output, attention_weights),\n        # # torchinfo might need to know which element is the primary output.\n        # # It usually handles this by taking the first element of a tuple.\n        # print(summary(model, input_data=(dummy_input_ids_summary, dummy_attention_mask_summary), device=str(device)))\n\n        model_save_dir = os.path.join(model_save_base_dir, full_model_name)\n\n        # Train model\n        best_model, training_time_seconds = train_model(model, learning_rate, epochs, train_dataloader, val_dataloader, model_save_dir=model_save_dir)\n        \n        # Get statistics\n        results = get_statistics(best_model, test_dataloader, 0.5)\n        accuracy = results['accuracy']\n        precision = results['precision']\n        recall = results['recall']\n        f1_score = results['f1_score']\n        confusion_matrix_array = results['confusion_matrix']\n        cm_tn, cm_fp, cm_fn, cm_tp = confusion_matrix_array.ravel()\n\n        # Prepare log line\n        log_line = (f\"{full_model_name},\"\n                    f\"{accuracy:.4f},\"\n                    f\"{precision:.4f},\"\n                    f\"{recall:.4f},\"\n                    f\"{f1_score:.4f},\"\n                    f\"{cm_tn},\"\n                    f\"{cm_fp},\"\n                    f\"{cm_fn},\"\n                    f\"{cm_tp},\"\n                    f\"{training_time_seconds:.2f},\" # New: Training Time\n                    f\"{num_trainable_params}\\n\")     # New: Trainable Parameters\n\n        # Append to log file\n        with open(results_log_file_path, 'a') as f_log:\n            f_log.write(log_line)\n        \n        print(f\"Logged results for {full_model_name} to {results_log_file_path}\")\n        print(f\"--- Finished training and logging for model: {full_model_name} ---\")\n\nprint(\"\\nAll models trained and results logged.\")\nprint(f\"Final results available at: {results_log_file_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T19:36:14.935221Z","iopub.execute_input":"2025-07-08T19:36:14.935507Z","iopub.status.idle":"2025-07-08T20:45:50.583322Z","shell.execute_reply.started":"2025-07-08T19:36:14.935487Z","shell.execute_reply":"2025-07-08T20:45:50.582554Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting training and evaluation for Attention-Based Models ---\n\n--- Training VanillaRNNWithBahdanauAttention ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaRNNWithBahdanauAttention\nEpoch 1/30 | Train Loss: 0.6240\nEpoch 1/30 | Val Loss: 0.5591\nValidation loss improved. Current best: 0.5591 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.5463\nEpoch 2/30 | Val Loss: 0.6263\nValidation loss did not improve for 1 epoch(s). Best was 0.5591 at Epoch 1.\nEpoch 3/30 | Train Loss: 0.5511\nEpoch 3/30 | Val Loss: 0.5926\nValidation loss did not improve for 2 epoch(s). Best was 0.5591 at Epoch 1.\nEpoch 4/30 | Train Loss: 0.5400\nEpoch 4/30 | Val Loss: 0.4995\nValidation loss improved. Current best: 0.4995 (Epoch 4).\nEpoch 5/30 | Train Loss: 0.4946\nEpoch 5/30 | Val Loss: 0.6960\nValidation loss did not improve for 1 epoch(s). Best was 0.4995 at Epoch 4.\nEpoch 6/30 | Train Loss: 0.4621\nEpoch 6/30 | Val Loss: 0.4293\nValidation loss improved. Current best: 0.4293 (Epoch 6).\nEpoch 7/30 | Train Loss: 0.4711\nEpoch 7/30 | Val Loss: 0.4576\nValidation loss did not improve for 1 epoch(s). Best was 0.4293 at Epoch 6.\nEpoch 8/30 | Train Loss: 0.4525\nEpoch 8/30 | Val Loss: 0.4529\nValidation loss did not improve for 2 epoch(s). Best was 0.4293 at Epoch 6.\nEpoch 9/30 | Train Loss: 0.4298\nEpoch 9/30 | Val Loss: 0.5305\nValidation loss did not improve for 3 epoch(s). Best was 0.4293 at Epoch 6.\nEpoch 10/30 | Train Loss: 0.4399\nEpoch 10/30 | Val Loss: 0.5813\nValidation loss did not improve for 4 epoch(s). Best was 0.4293 at Epoch 6.\nEpoch 11/30 | Train Loss: 0.4215\nEpoch 11/30 | Val Loss: 0.4412\nValidation loss did not improve for 5 epoch(s). Best was 0.4293 at Epoch 6.\nEarly stopping triggered after 11 epochs (patience: 5).\nFinished training for VanillaRNNWithPlugAndPlayAttention. Total time: 282.26 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaRNNWithBahdanauAttention/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaRNNWithBahdanauAttention/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaRNNWithBahdanauAttention/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.7902\nPrecision: 0.7272\nRecall: 0.9354\nF1-score: 0.8183\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                    1589                     886\nActual Positive (1)                     163                    2362\nLogged results for VanillaRNNWithBahdanauAttention to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaRNNWithBahdanauAttention ---\n\n--- Training VanillaRNNWithLuongDotProductAttention ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaRNNWithLuongDotProductAttention\nEpoch 1/30 | Train Loss: 0.7046\nEpoch 1/30 | Val Loss: 0.6532\nValidation loss improved. Current best: 0.6532 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.6970\nEpoch 2/30 | Val Loss: 0.6568\nValidation loss did not improve for 1 epoch(s). Best was 0.6532 at Epoch 1.\nEpoch 3/30 | Train Loss: 0.6883\nEpoch 3/30 | Val Loss: 0.7093\nValidation loss did not improve for 2 epoch(s). Best was 0.6532 at Epoch 1.\nEpoch 4/30 | Train Loss: 0.6786\nEpoch 4/30 | Val Loss: 0.7040\nValidation loss did not improve for 3 epoch(s). Best was 0.6532 at Epoch 1.\nEpoch 5/30 | Train Loss: 0.7379\nEpoch 5/30 | Val Loss: 0.7354\nValidation loss did not improve for 4 epoch(s). Best was 0.6532 at Epoch 1.\nEpoch 6/30 | Train Loss: 0.7249\nEpoch 6/30 | Val Loss: 0.6938\nValidation loss did not improve for 5 epoch(s). Best was 0.6532 at Epoch 1.\nEarly stopping triggered after 6 epochs (patience: 5).\nFinished training for VanillaRNNWithPlugAndPlayAttention. Total time: 142.14 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaRNNWithLuongDotProductAttention/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaRNNWithLuongDotProductAttention/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaRNNWithLuongDotProductAttention/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.5052\nPrecision: 0.5051\nRecall: 1.0000\nF1-score: 0.6712\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                       1                    2474\nActual Positive (1)                       0                    2525\nLogged results for VanillaRNNWithLuongDotProductAttention to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaRNNWithLuongDotProductAttention ---\n\n--- Training VanillaRNNWithLuongGeneralAttention ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaRNNWithLuongGeneralAttention\nEpoch 1/30 | Train Loss: 0.6992\nEpoch 1/30 | Val Loss: 0.6665\nValidation loss improved. Current best: 0.6665 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.6838\nEpoch 2/30 | Val Loss: 0.6704\nValidation loss did not improve for 1 epoch(s). Best was 0.6665 at Epoch 1.\nEpoch 3/30 | Train Loss: 0.6744\nEpoch 3/30 | Val Loss: 0.6885\nValidation loss did not improve for 2 epoch(s). Best was 0.6665 at Epoch 1.\nEpoch 4/30 | Train Loss: 0.6917\nEpoch 4/30 | Val Loss: 0.6640\nValidation loss improved. Current best: 0.6640 (Epoch 4).\nEpoch 5/30 | Train Loss: 0.6753\nEpoch 5/30 | Val Loss: 0.6851\nValidation loss did not improve for 1 epoch(s). Best was 0.6640 at Epoch 4.\nEpoch 6/30 | Train Loss: 0.6809\nEpoch 6/30 | Val Loss: 0.6523\nValidation loss improved. Current best: 0.6523 (Epoch 6).\nEpoch 7/30 | Train Loss: 0.6676\nEpoch 7/30 | Val Loss: 0.6600\nValidation loss did not improve for 1 epoch(s). Best was 0.6523 at Epoch 6.\nEpoch 8/30 | Train Loss: 0.6642\nEpoch 8/30 | Val Loss: 0.6620\nValidation loss did not improve for 2 epoch(s). Best was 0.6523 at Epoch 6.\nEpoch 9/30 | Train Loss: 0.6577\nEpoch 9/30 | Val Loss: 0.6978\nValidation loss did not improve for 3 epoch(s). Best was 0.6523 at Epoch 6.\nEpoch 10/30 | Train Loss: 0.6449\nEpoch 10/30 | Val Loss: 0.8333\nValidation loss did not improve for 4 epoch(s). Best was 0.6523 at Epoch 6.\nEpoch 11/30 | Train Loss: 0.6453\nEpoch 11/30 | Val Loss: 0.6399\nValidation loss improved. Current best: 0.6399 (Epoch 11).\nEpoch 12/30 | Train Loss: 0.6449\nEpoch 12/30 | Val Loss: 0.7459\nValidation loss did not improve for 1 epoch(s). Best was 0.6399 at Epoch 11.\nEpoch 13/30 | Train Loss: 0.6575\nEpoch 13/30 | Val Loss: 0.6625\nValidation loss did not improve for 2 epoch(s). Best was 0.6399 at Epoch 11.\nEpoch 14/30 | Train Loss: 0.6632\nEpoch 14/30 | Val Loss: 0.6625\nValidation loss did not improve for 3 epoch(s). Best was 0.6399 at Epoch 11.\nEpoch 15/30 | Train Loss: 0.6508\nEpoch 15/30 | Val Loss: 0.6801\nValidation loss did not improve for 4 epoch(s). Best was 0.6399 at Epoch 11.\nEpoch 16/30 | Train Loss: 0.6445\nEpoch 16/30 | Val Loss: 0.6576\nValidation loss did not improve for 5 epoch(s). Best was 0.6399 at Epoch 11.\nEarly stopping triggered after 16 epochs (patience: 5).\nFinished training for VanillaRNNWithPlugAndPlayAttention. Total time: 380.48 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaRNNWithLuongGeneralAttention/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaRNNWithLuongGeneralAttention/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaRNNWithLuongGeneralAttention/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.6110\nPrecision: 0.6390\nRecall: 0.5279\nF1-score: 0.5782\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                    1722                     753\nActual Positive (1)                    1192                    1333\nLogged results for VanillaRNNWithLuongGeneralAttention to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaRNNWithLuongGeneralAttention ---\n\n--- Training VanillaRNNWithLuongConcatAttention ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaRNNWithLuongConcatAttention\nEpoch 1/30 | Train Loss: 0.6293\nEpoch 1/30 | Val Loss: 0.5314\nValidation loss improved. Current best: 0.5314 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.5309\nEpoch 2/30 | Val Loss: 0.5319\nValidation loss did not improve for 1 epoch(s). Best was 0.5314 at Epoch 1.\nEpoch 3/30 | Train Loss: 0.4793\nEpoch 3/30 | Val Loss: 0.4804\nValidation loss improved. Current best: 0.4804 (Epoch 3).\nEpoch 4/30 | Train Loss: 0.5089\nEpoch 4/30 | Val Loss: 0.4607\nValidation loss improved. Current best: 0.4607 (Epoch 4).\nEpoch 5/30 | Train Loss: 0.4582\nEpoch 5/30 | Val Loss: 0.4901\nValidation loss did not improve for 1 epoch(s). Best was 0.4607 at Epoch 4.\nEpoch 6/30 | Train Loss: 0.4271\nEpoch 6/30 | Val Loss: 0.4273\nValidation loss improved. Current best: 0.4273 (Epoch 6).\nEpoch 7/30 | Train Loss: 0.4117\nEpoch 7/30 | Val Loss: 0.3903\nValidation loss improved. Current best: 0.3903 (Epoch 7).\nEpoch 8/30 | Train Loss: 0.4128\nEpoch 8/30 | Val Loss: 0.4286\nValidation loss did not improve for 1 epoch(s). Best was 0.3903 at Epoch 7.\nEpoch 9/30 | Train Loss: 0.3979\nEpoch 9/30 | Val Loss: 0.3941\nValidation loss did not improve for 2 epoch(s). Best was 0.3903 at Epoch 7.\nEpoch 10/30 | Train Loss: 0.3880\nEpoch 10/30 | Val Loss: 0.3933\nValidation loss did not improve for 3 epoch(s). Best was 0.3903 at Epoch 7.\nEpoch 11/30 | Train Loss: 0.3828\nEpoch 11/30 | Val Loss: 0.4021\nValidation loss did not improve for 4 epoch(s). Best was 0.3903 at Epoch 7.\nEpoch 12/30 | Train Loss: 0.3818\nEpoch 12/30 | Val Loss: 0.4906\nValidation loss did not improve for 5 epoch(s). Best was 0.3903 at Epoch 7.\nEarly stopping triggered after 12 epochs (patience: 5).\nFinished training for VanillaRNNWithPlugAndPlayAttention. Total time: 335.73 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaRNNWithLuongConcatAttention/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaRNNWithLuongConcatAttention/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaRNNWithLuongConcatAttention/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.7798\nPrecision: 0.9289\nRecall: 0.6107\nF1-score: 0.7369\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                    2357                     118\nActual Positive (1)                     983                    1542\nLogged results for VanillaRNNWithLuongConcatAttention to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaRNNWithLuongConcatAttention ---\n\n--- Training VanillaBidirectionalRNNWithBahdanauAttention ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaBidirectionalRNNWithBahdanauAttention\nEpoch 1/30 | Train Loss: 0.5531\nEpoch 1/30 | Val Loss: 0.4691\nValidation loss improved. Current best: 0.4691 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.4596\nEpoch 2/30 | Val Loss: 0.4390\nValidation loss improved. Current best: 0.4390 (Epoch 2).\nEpoch 3/30 | Train Loss: 0.4494\nEpoch 3/30 | Val Loss: 0.5126\nValidation loss did not improve for 1 epoch(s). Best was 0.4390 at Epoch 2.\nEpoch 4/30 | Train Loss: 0.4359\nEpoch 4/30 | Val Loss: 0.6735\nValidation loss did not improve for 2 epoch(s). Best was 0.4390 at Epoch 2.\nEpoch 5/30 | Train Loss: 0.4382\nEpoch 5/30 | Val Loss: 0.4246\nValidation loss improved. Current best: 0.4246 (Epoch 5).\nEpoch 6/30 | Train Loss: 0.4308\nEpoch 6/30 | Val Loss: 0.4118\nValidation loss improved. Current best: 0.4118 (Epoch 6).\nEpoch 7/30 | Train Loss: 0.4142\nEpoch 7/30 | Val Loss: 0.4254\nValidation loss did not improve for 1 epoch(s). Best was 0.4118 at Epoch 6.\nEpoch 8/30 | Train Loss: 0.4103\nEpoch 8/30 | Val Loss: 0.6145\nValidation loss did not improve for 2 epoch(s). Best was 0.4118 at Epoch 6.\nEpoch 9/30 | Train Loss: 0.4283\nEpoch 9/30 | Val Loss: 0.4294\nValidation loss did not improve for 3 epoch(s). Best was 0.4118 at Epoch 6.\nEpoch 10/30 | Train Loss: 0.4355\nEpoch 10/30 | Val Loss: 0.3952\nValidation loss improved. Current best: 0.3952 (Epoch 10).\nEpoch 11/30 | Train Loss: 0.4199\nEpoch 11/30 | Val Loss: 0.3870\nValidation loss improved. Current best: 0.3870 (Epoch 11).\nEpoch 12/30 | Train Loss: 0.4105\nEpoch 12/30 | Val Loss: 0.4090\nValidation loss did not improve for 1 epoch(s). Best was 0.3870 at Epoch 11.\nEpoch 13/30 | Train Loss: 0.4300\nEpoch 13/30 | Val Loss: 0.3960\nValidation loss did not improve for 2 epoch(s). Best was 0.3870 at Epoch 11.\nEpoch 14/30 | Train Loss: 0.4303\nEpoch 14/30 | Val Loss: 0.4784\nValidation loss did not improve for 3 epoch(s). Best was 0.3870 at Epoch 11.\nEpoch 15/30 | Train Loss: 0.4172\nEpoch 15/30 | Val Loss: 0.4359\nValidation loss did not improve for 4 epoch(s). Best was 0.3870 at Epoch 11.\nEpoch 16/30 | Train Loss: 0.4261\nEpoch 16/30 | Val Loss: 0.5140\nValidation loss did not improve for 5 epoch(s). Best was 0.3870 at Epoch 11.\nEarly stopping triggered after 16 epochs (patience: 5).\nFinished training for VanillaBidirectionalRNNWithPlugAndPlayAttention. Total time: 705.85 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithBahdanauAttention/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithBahdanauAttention/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithBahdanauAttention/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.7790\nPrecision: 0.7133\nRecall: 0.9402\nF1-score: 0.8112\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                    1521                     954\nActual Positive (1)                     151                    2374\nLogged results for VanillaBidirectionalRNNWithBahdanauAttention to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaBidirectionalRNNWithBahdanauAttention ---\n\n--- Training VanillaBidirectionalRNNWithLuongDotProductAttention ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongDotProductAttention\nEpoch 1/30 | Train Loss: 0.7588\nEpoch 1/30 | Val Loss: 0.7357\nValidation loss improved. Current best: 0.7357 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.7704\nEpoch 2/30 | Val Loss: 0.7494\nValidation loss did not improve for 1 epoch(s). Best was 0.7357 at Epoch 1.\nEpoch 3/30 | Train Loss: 0.7886\nEpoch 3/30 | Val Loss: 0.8850\nValidation loss did not improve for 2 epoch(s). Best was 0.7357 at Epoch 1.\nEpoch 4/30 | Train Loss: 0.7796\nEpoch 4/30 | Val Loss: 0.7309\nValidation loss improved. Current best: 0.7309 (Epoch 4).\nEpoch 5/30 | Train Loss: 0.7756\nEpoch 5/30 | Val Loss: 0.9048\nValidation loss did not improve for 1 epoch(s). Best was 0.7309 at Epoch 4.\nEpoch 6/30 | Train Loss: 0.7715\nEpoch 6/30 | Val Loss: 0.7465\nValidation loss did not improve for 2 epoch(s). Best was 0.7309 at Epoch 4.\nEpoch 7/30 | Train Loss: 0.7511\nEpoch 7/30 | Val Loss: 0.8507\nValidation loss did not improve for 3 epoch(s). Best was 0.7309 at Epoch 4.\nEpoch 8/30 | Train Loss: 0.7622\nEpoch 8/30 | Val Loss: 0.7114\nValidation loss improved. Current best: 0.7114 (Epoch 8).\nEpoch 9/30 | Train Loss: 0.7708\nEpoch 9/30 | Val Loss: 0.7875\nValidation loss did not improve for 1 epoch(s). Best was 0.7114 at Epoch 8.\nEpoch 10/30 | Train Loss: 0.7336\nEpoch 10/30 | Val Loss: 0.6416\nValidation loss improved. Current best: 0.6416 (Epoch 10).\nEpoch 11/30 | Train Loss: 0.7384\nEpoch 11/30 | Val Loss: 0.9959\nValidation loss did not improve for 1 epoch(s). Best was 0.6416 at Epoch 10.\nEpoch 12/30 | Train Loss: 0.7441\nEpoch 12/30 | Val Loss: 0.6823\nValidation loss did not improve for 2 epoch(s). Best was 0.6416 at Epoch 10.\nEpoch 13/30 | Train Loss: 0.7287\nEpoch 13/30 | Val Loss: 0.7729\nValidation loss did not improve for 3 epoch(s). Best was 0.6416 at Epoch 10.\nEpoch 14/30 | Train Loss: 0.7265\nEpoch 14/30 | Val Loss: 0.6758\nValidation loss did not improve for 4 epoch(s). Best was 0.6416 at Epoch 10.\nEpoch 15/30 | Train Loss: 0.7031\nEpoch 15/30 | Val Loss: 0.8097\nValidation loss did not improve for 5 epoch(s). Best was 0.6416 at Epoch 10.\nEarly stopping triggered after 15 epochs (patience: 5).\nFinished training for VanillaBidirectionalRNNWithPlugAndPlayAttention. Total time: 609.17 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongDotProductAttention/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongDotProductAttention/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongDotProductAttention/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.6120\nPrecision: 0.5746\nRecall: 0.8923\nF1-score: 0.6990\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                     807                    1668\nActual Positive (1)                     272                    2253\nLogged results for VanillaBidirectionalRNNWithLuongDotProductAttention to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaBidirectionalRNNWithLuongDotProductAttention ---\n\n--- Training VanillaBidirectionalRNNWithLuongGeneralAttention ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongGeneralAttention\nEpoch 1/30 | Train Loss: 0.8023\nEpoch 1/30 | Val Loss: 0.7521\nValidation loss improved. Current best: 0.7521 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.7927\nEpoch 2/30 | Val Loss: 0.7600\nValidation loss did not improve for 1 epoch(s). Best was 0.7521 at Epoch 1.\nEpoch 3/30 | Train Loss: 0.7887\nEpoch 3/30 | Val Loss: 0.8376\nValidation loss did not improve for 2 epoch(s). Best was 0.7521 at Epoch 1.\nEpoch 4/30 | Train Loss: 0.7666\nEpoch 4/30 | Val Loss: 0.7080\nValidation loss improved. Current best: 0.7080 (Epoch 4).\nEpoch 5/30 | Train Loss: 0.7785\nEpoch 5/30 | Val Loss: 0.7364\nValidation loss did not improve for 1 epoch(s). Best was 0.7080 at Epoch 4.\nEpoch 6/30 | Train Loss: 0.7729\nEpoch 6/30 | Val Loss: 0.7214\nValidation loss did not improve for 2 epoch(s). Best was 0.7080 at Epoch 4.\nEpoch 7/30 | Train Loss: 0.7540\nEpoch 7/30 | Val Loss: 1.0378\nValidation loss did not improve for 3 epoch(s). Best was 0.7080 at Epoch 4.\nEpoch 8/30 | Train Loss: 0.7395\nEpoch 8/30 | Val Loss: 0.6921\nValidation loss improved. Current best: 0.6921 (Epoch 8).\nEpoch 9/30 | Train Loss: 0.7343\nEpoch 9/30 | Val Loss: 0.7054\nValidation loss did not improve for 1 epoch(s). Best was 0.6921 at Epoch 8.\nEpoch 10/30 | Train Loss: 0.7239\nEpoch 10/30 | Val Loss: 0.7449\nValidation loss did not improve for 2 epoch(s). Best was 0.6921 at Epoch 8.\nEpoch 11/30 | Train Loss: 0.7133\nEpoch 11/30 | Val Loss: 0.7022\nValidation loss did not improve for 3 epoch(s). Best was 0.6921 at Epoch 8.\nEpoch 12/30 | Train Loss: 0.7033\nEpoch 12/30 | Val Loss: 0.7052\nValidation loss did not improve for 4 epoch(s). Best was 0.6921 at Epoch 8.\nEpoch 13/30 | Train Loss: 0.7227\nEpoch 13/30 | Val Loss: 0.7767\nValidation loss did not improve for 5 epoch(s). Best was 0.6921 at Epoch 8.\nEarly stopping triggered after 13 epochs (patience: 5).\nFinished training for VanillaBidirectionalRNNWithPlugAndPlayAttention. Total time: 585.15 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongGeneralAttention/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongGeneralAttention/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongGeneralAttention/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.5588\nPrecision: 0.5835\nRecall: 0.4416\nF1-score: 0.5027\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                    1679                     796\nActual Positive (1)                    1410                    1115\nLogged results for VanillaBidirectionalRNNWithLuongGeneralAttention to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaBidirectionalRNNWithLuongGeneralAttention ---\n\n--- Training VanillaBidirectionalRNNWithLuongConcatAttention ---\nModel saving directory created/ensured: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongConcatAttention\nEpoch 1/30 | Train Loss: 0.5883\nEpoch 1/30 | Val Loss: 0.5706\nValidation loss improved. Current best: 0.5706 (Epoch 1).\nEpoch 2/30 | Train Loss: 0.5539\nEpoch 2/30 | Val Loss: 0.5079\nValidation loss improved. Current best: 0.5079 (Epoch 2).\nEpoch 3/30 | Train Loss: 0.5377\nEpoch 3/30 | Val Loss: 0.5312\nValidation loss did not improve for 1 epoch(s). Best was 0.5079 at Epoch 2.\nEpoch 4/30 | Train Loss: 0.5410\nEpoch 4/30 | Val Loss: 0.5520\nValidation loss did not improve for 2 epoch(s). Best was 0.5079 at Epoch 2.\nEpoch 5/30 | Train Loss: 0.5085\nEpoch 5/30 | Val Loss: 0.4520\nValidation loss improved. Current best: 0.4520 (Epoch 5).\nEpoch 6/30 | Train Loss: 0.4709\nEpoch 6/30 | Val Loss: 0.4968\nValidation loss did not improve for 1 epoch(s). Best was 0.4520 at Epoch 5.\nEpoch 7/30 | Train Loss: 0.4669\nEpoch 7/30 | Val Loss: 0.5053\nValidation loss did not improve for 2 epoch(s). Best was 0.4520 at Epoch 5.\nEpoch 8/30 | Train Loss: 0.4579\nEpoch 8/30 | Val Loss: 0.4702\nValidation loss did not improve for 3 epoch(s). Best was 0.4520 at Epoch 5.\nEpoch 9/30 | Train Loss: 0.4656\nEpoch 9/30 | Val Loss: 0.4404\nValidation loss improved. Current best: 0.4404 (Epoch 9).\nEpoch 10/30 | Train Loss: 0.4684\nEpoch 10/30 | Val Loss: 0.4359\nValidation loss improved. Current best: 0.4359 (Epoch 10).\nEpoch 11/30 | Train Loss: 0.4492\nEpoch 11/30 | Val Loss: 0.4788\nValidation loss did not improve for 1 epoch(s). Best was 0.4359 at Epoch 10.\nEpoch 12/30 | Train Loss: 0.4252\nEpoch 12/30 | Val Loss: 0.4204\nValidation loss improved. Current best: 0.4204 (Epoch 12).\nEpoch 13/30 | Train Loss: 0.4181\nEpoch 13/30 | Val Loss: 0.5252\nValidation loss did not improve for 1 epoch(s). Best was 0.4204 at Epoch 12.\nEpoch 14/30 | Train Loss: 0.4237\nEpoch 14/30 | Val Loss: 0.4108\nValidation loss improved. Current best: 0.4108 (Epoch 14).\nEpoch 15/30 | Train Loss: 0.4101\nEpoch 15/30 | Val Loss: 0.5264\nValidation loss did not improve for 1 epoch(s). Best was 0.4108 at Epoch 14.\nEpoch 16/30 | Train Loss: 0.4016\nEpoch 16/30 | Val Loss: 0.4367\nValidation loss did not improve for 2 epoch(s). Best was 0.4108 at Epoch 14.\nEpoch 17/30 | Train Loss: 0.4214\nEpoch 17/30 | Val Loss: 0.4046\nValidation loss improved. Current best: 0.4046 (Epoch 17).\nEpoch 18/30 | Train Loss: 0.4105\nEpoch 18/30 | Val Loss: 0.4120\nValidation loss did not improve for 1 epoch(s). Best was 0.4046 at Epoch 17.\nEpoch 19/30 | Train Loss: 0.4128\nEpoch 19/30 | Val Loss: 0.3918\nValidation loss improved. Current best: 0.3918 (Epoch 19).\nEpoch 20/30 | Train Loss: 0.4022\nEpoch 20/30 | Val Loss: 0.4643\nValidation loss did not improve for 1 epoch(s). Best was 0.3918 at Epoch 19.\nEpoch 21/30 | Train Loss: 0.4071\nEpoch 21/30 | Val Loss: 0.4957\nValidation loss did not improve for 2 epoch(s). Best was 0.3918 at Epoch 19.\nEpoch 22/30 | Train Loss: 0.3912\nEpoch 22/30 | Val Loss: 0.4054\nValidation loss did not improve for 3 epoch(s). Best was 0.3918 at Epoch 19.\nEpoch 23/30 | Train Loss: 0.3971\nEpoch 23/30 | Val Loss: 0.4370\nValidation loss did not improve for 4 epoch(s). Best was 0.3918 at Epoch 19.\nEpoch 24/30 | Train Loss: 0.4038\nEpoch 24/30 | Val Loss: 0.4342\nValidation loss did not improve for 5 epoch(s). Best was 0.3918 at Epoch 19.\nEarly stopping triggered after 24 epochs (patience: 5).\nFinished training for VanillaBidirectionalRNNWithPlugAndPlayAttention. Total time: 1113.54 seconds.\nBest model (based on validation loss) saved successfully to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongConcatAttention/best_model.pth\nTraining logs saved to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongConcatAttention/training_log.csv\nLoss plot saved to: /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongConcatAttention/loss_plot.png\nLoaded best model state for return.\n\n--- Classification Statistics ---\nAccuracy: 0.8096\nPrecision: 0.7523\nRecall: 0.9287\nF1-score: 0.8313\n\nConfusion Matrix:\n                     Predicted Negative (0)  Predicted Positive (1)\nActual Negative (0)                    1703                     772\nActual Positive (1)                     180                    2345\nLogged results for VanillaBidirectionalRNNWithLuongConcatAttention to /kaggle/working/model_results.csv\n--- Finished training and logging for model: VanillaBidirectionalRNNWithLuongConcatAttention ---\n\nAll models trained and results logged.\nFinal results available at: /kaggle/working/model_results.csv\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"! zip -r models_with_attention /kaggle/working/saved_models/VanillaRNNWithBahdanauAttention /kaggle/working/saved_models/VanillaRNNWithLuongConcatAttention /kaggle/working/saved_models/VanillaRNNWithLuongDotProductAttention /kaggle/working/saved_models/VanillaRNNWithLuongGeneralAttention /kaggle/working/saved_models/VanillaBidirectionalRNNWithBahdanauAttention /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongConcatAttention /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongDotProductAttention /kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongGeneralAttention \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T20:45:58.780630Z","iopub.execute_input":"2025-07-08T20:45:58.780925Z","iopub.status.idle":"2025-07-08T20:46:20.348789Z","shell.execute_reply.started":"2025-07-08T20:45:58.780903Z","shell.execute_reply":"2025-07-08T20:46:20.348100Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/saved_models/VanillaRNNWithBahdanauAttention/ (stored 0%)\n  adding: kaggle/working/saved_models/VanillaRNNWithBahdanauAttention/training_log.csv (deflated 44%)\n  adding: kaggle/working/saved_models/VanillaRNNWithBahdanauAttention/loss_plot.png (deflated 7%)\n  adding: kaggle/working/saved_models/VanillaRNNWithBahdanauAttention/best_model.pth (deflated 7%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongConcatAttention/ (stored 0%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongConcatAttention/training_log.csv (deflated 46%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongConcatAttention/loss_plot.png (deflated 10%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongConcatAttention/best_model.pth (deflated 7%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongDotProductAttention/ (stored 0%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongDotProductAttention/training_log.csv (deflated 39%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongDotProductAttention/loss_plot.png (deflated 7%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongDotProductAttention/best_model.pth (deflated 7%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongGeneralAttention/ (stored 0%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongGeneralAttention/training_log.csv (deflated 48%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongGeneralAttention/loss_plot.png (deflated 8%)\n  adding: kaggle/working/saved_models/VanillaRNNWithLuongGeneralAttention/best_model.pth (deflated 7%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithBahdanauAttention/ (stored 0%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithBahdanauAttention/training_log.csv (deflated 48%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithBahdanauAttention/loss_plot.png (deflated 6%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithBahdanauAttention/best_model.pth (deflated 7%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongConcatAttention/ (stored 0%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongConcatAttention/training_log.csv (deflated 49%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongConcatAttention/loss_plot.png (deflated 9%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongConcatAttention/best_model.pth (deflated 7%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongDotProductAttention/ (stored 0%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongDotProductAttention/training_log.csv (deflated 46%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongDotProductAttention/loss_plot.png (deflated 6%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongDotProductAttention/best_model.pth (deflated 7%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongGeneralAttention/ (stored 0%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongGeneralAttention/training_log.csv (deflated 45%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongGeneralAttention/loss_plot.png (deflated 9%)\n  adding: kaggle/working/saved_models/VanillaBidirectionalRNNWithLuongGeneralAttention/best_model.pth (deflated 7%)\n","output_type":"stream"}],"execution_count":94}]}